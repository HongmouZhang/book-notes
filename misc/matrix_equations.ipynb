{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of equations related to matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Inverse\n",
    "\n",
    "There are several non-trivial matrix inverse equations.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(P^{-1} + B^TR^{-1}B)^{-1} &= P - PB^T(BPB^T + R)^{-1} BP \\\\\n",
    "(P^{-1} + B^TR^{-1}B)^{-1} B^T R^{-1} &= PB^T(BPB^T + R)^{-1} \\\\\n",
    "(A + BD^{-1}C)^{-1} &= A^{-1} - A^{-1} B(D+CA^{-1}B)^{-1} CA^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "First two are from [Max Welling's note for Kalman Filters (KF)](http://www.ics.uci.edu/~welling/classnotes/papers_class/KF.ps.gz) in <http://www.ics.uci.edu/~welling/classnotes/classnotes.html>, and last two are from Appendix C of PRML (So second one appears in both).\n",
    "\n",
    "The second is easy to verify, by multiplying two sides by  $(P^{-1} + B^TR^{-1}B)$ and $(BPB^T + R)$. The proof of the third one (also called [Woodbury matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity)) is at <https://en.wikipedia.org/wiki/Woodbury_matrix_identity#Direct_proof>. Just verify it by brute force. The first one is actually the same as the third one.\n",
    "\n",
    "Another popular one is a block matrix inversion equation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{matrix} A & B \\\\ C & D \\end{matrix}\\right]^{-1} = \\left[ \\begin{matrix} \\left(A-B D^{-1} C \\right)^{-1}  &   -\\left(A-B D^{-1} C \\right)^{-1} B D^{-1} \\\\ -D^{-1}C\\left(A-B D^{-1} C \\right)^{-1} & D^{-1}+ D^{-1} C \\left(A-B D^{-1} C \\right)^{-1} B D^{-1} \\end{matrix} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and $\\left(A-B D^{-1} C \\right)$ is usually called the Schur complement of the whole matrix $\\left[ \\begin{matrix} A & B \\\\ C & D \\end{matrix}\\right]$ w.r.t. $D$.\n",
    "\n",
    "This appears in multiple places, such as <https://en.wikipedia.org/wiki/Schur_complement#Background>, Eq. 2.76 of PRML, Section 4.3.4 of MLAPP, etc.\n",
    "\n",
    "In Section 4.3.4 of MLAPP, there is an alternative version, using Schur complement w.r.t. $A$, and 4.3.4.2 of MLAPP have more equations related to block matrix inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix representation of computing covariance matrix\n",
    "\n",
    "Given data matrix of size $n \\times p$, we want to compute its covariance matrix. To do this, we should compute the mean feature vector $\\mu$ of size $p$, subtract it from every row of $X$, denoted as $X_c$, and then compute $\\frac{1}{n} X_c^T X_c$. How to represent this in matrix operations? The trick is a matrix with all 1 of size $n \\times n$, denoted as $\\mathbf{1}_n$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_c &= (I - \\frac{1}{n} \\mathbf{1}_n) X \\\\\n",
    "X_c^T X_c   &=  X^T(I - \\frac{1}{n} \\mathbf{1}_n)^T (I - \\frac{1}{n} \\mathbf{1}_n) X \\\\\n",
    "            &=  X^T(I - \\frac{1}{n} \\mathbf{1}_n)(I - \\frac{1}{n} \\mathbf{1}_n) X \\\\\n",
    "            &=  X^T X -2 X^T  \\frac{1}{n} \\mathbf{1}_n X +  X^T \\frac{1}{n^2} \\mathbf{1}_n \\mathbf{1}_n X  \\\\\n",
    "            &=  X^T X -2 X^T  \\frac{1}{n} \\mathbf{1}_n X +  X^T \\frac{1}{n} \\mathbf{1}_n X \\\\\n",
    "            &=  X^T X - X^T  \\frac{1}{n} \\mathbf{1}_n X  \\\\\n",
    "            &=  X^T X - X^T (\\frac{1}{n} \\mathbf{1}_n)^T  \\frac{1}{n} \\mathbf{1}_n X \\\\\n",
    "            &=  X^T X - \\begin{bmatrix} \\mu & \\mu & \\ldots & \\mu \\end{bmatrix} \\begin{bmatrix} \\mu^T \\\\ \\mu^T \\\\ \\ldots \\\\ \\mu^T \\end{bmatrix} \\\\\n",
    "            &=  X^T X - n \\mu \\mu^T \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{1}{n}X_c^T X_c   &=  \\frac{1}{n}X^T X - \\mu \\mu^T\n",
    "\\end{align}\n",
    "$$\n",
    "where some lines, such as 5th and 7th, depend on the fact that $\\frac{1}{n}\\mathbf{1}_n$ multiplied by itself is itself. You can verify this very easily\n",
    "\n",
    "Similar matrix operations for centered kernel (pairwise distance) matrix (so the result is $n \\times n$, not $p \\times p$) can be found in pp. 671 (18.5.2) of ESLII and <http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
