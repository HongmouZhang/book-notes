{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.3 Parameter estimation for mixture models\n",
    "\n",
    "Based on text here and Figure 11.9, we can see that the main reason $\\theta_z$ and $\\theta_z$ being coupled is due to V-structured interaction. If $z$'s are also observed, then they will decouple and learning becomes easy.\n",
    "\n",
    "Above Eq. (11.14), the author claims that \"mixtures of exponential families are also in the exponential family, providing the mixing indicator variables are observed (Exercise 11.1)\". If you check Exercise 11.1, you may find that's not what you want. What it really means is that the indicator and x jointly belong to exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.4 The EM algorithm\n",
    "\n",
    "Text below Eq. (11.16) says that while we can in theory simply perform gradient descent on latent variable models, EM algorithm can deal with constraints such as matrix being positive definite, and can often take large update steps. Thus EM is probably better than being brute-force.\n",
    "\n",
    "### 11.4.2.7 Initialization and avoiding local minima\n",
    "\n",
    "this section introduces some tricks on how to avoid bad local minima, using kmeans++, or by starting with few clusters and splitting them.\n",
    "\n",
    "### 11.4.8-11.4.9 Online EM and other EM variants\n",
    "\n",
    "Essentially, these add bell and whistles to EM, either approximating E or M step, or changing the way coordinate descent is performed. (as said in 11.4.7 or Section 19.2 of DL book, EM is coordinate ascent on $\\theta$ and $q$; you can think of other methods, for example; don't do E step at all, but optimize each argument of $\\theta$ one at a time, etc; there are many ways coordinate ascent can be performed to ensure monotonic increase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.5 Model selection for latent variable models\n",
    "\n",
    "### 11.5.1 Model selection for probabilistic models\n",
    "\n",
    "Seems that apart from using cross-validated likelihood, all other approaches are approximations to computing model likelihood.\n",
    "\n",
    "### 11.5.2 Model selection for non-probabilistic models\n",
    "\n",
    "two points.\n",
    "\n",
    "1. For unsupervised learning, having a test set is useless for model selection. See Figure 11.20.\n",
    "2. probably we can do \"knee-finding\" (I think it's like finding the elbow for PCA models). Also, Section 12.3.2.1 has some other method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
