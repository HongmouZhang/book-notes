{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Sampling distribution of an estimator\n",
    "\n",
    "### pp. 196\n",
    "\n",
    "Here the large sample theory of MLE is not very clearly written, and I believe some definitions, such as the one below Eq. (6.4), is just wrong. Instead, for these properties, check Section 7.4 (Information and Efficiency) of [Modern Mathematical Statistics with Applications 2nd edition](dx.doi.org/10.1007/978-1-4614-0391-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Frequentist decision theory\n",
    "\n",
    "### pp. 197\n",
    "\n",
    "In Eq. (6.9) and Eq. (6.10), $L(\\theta, a)$ is defined as in (5.109), that is, the expected error over the distribution generated by $\\theta$, using decision function $a(x)$.\n",
    "\n",
    "The footnote 3 here is very insightful about why using frequentist approaches has limitations.\n",
    "\n",
    "> In practice, the frequentist approach is usually only applied to one-shot statistical decision problems — such as classification, regression and parameter estimation — since its non-constructive nature makes it difficult to apply to sequential decision problems, which adapt to data online.\n",
    "\n",
    "6.3.1 is just the Bayes decision rule, minimizing posterior loss for each input. Here, we call $p(\\theta)$ a prior. But in Eq. (5.110), the prior has incorporated the data, and the form is the same as here.\n",
    "\n",
    "6.3.2 Minimax is really pessimistic. It sounds to me like worst case analysis. Which is probably useless with out adversary.\n",
    "\n",
    "### pp. 201\n",
    "\n",
    "6.3.3.2 shows some peculiarities of MLE estimators. But I think in practice it's not a big problem. See [here](./chapter06/SteinParadox.pdf) (original link <http://www.statslab.cam.ac.uk/~rjs57/SteinParadox.pdf>) for some proof about this. Some observations in it are great.\n",
    "\n",
    "> Although Stein’s result is very clean to state and prove, it may seem somewhat removed from practical statistical problems. Nevertheless, the idea at the heart of Stein’s proposal, namely that of employing shrinkage to reduce variance (at the expense of introducing bias) turns out to be a very powerful one that has had a huge impact on statistical methodology.\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Stein's_example#An_intuitive_explanation> gives some explanation.\n",
    "\n",
    "> The trick of the Stein estimator, and others that yield the Stein paradox, is that they adjust the shift in such a way that there is always (for any $\\theta$ vector) at least one $X_{i}$ whose mean square error is improved, and its improvement more than compensates for any degradation in mean square error that might occur for another $\\hat{\\theta_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Desirable properties of estimators\n",
    "\n",
    "### pp. 207\n",
    "\n",
    "6.4.4.3 shows that bias-variance trade off analysis actually doesn't work for classification. Check the Hastie 2009 for detail. Still, I think bias-variance trade off is totally wrong for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Pathologies of frequentist statistics\n",
    "\n",
    "### pp. 216\n",
    "\n",
    "Very interesting argument saying that p-value depends on stopping rule!\n",
    "\n",
    "### pp. 217\n",
    "\n",
    "6.6.3 seems to suggest frequentist approaches actually depend on unseen data, which is also mentioned in [A practical solution to the pervasive problems of p values](http://www.ejwagenmakers.com/2007/pValueProblems.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
