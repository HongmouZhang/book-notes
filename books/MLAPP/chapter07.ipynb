{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Model specification\n",
    "\n",
    "### pp. 219\n",
    "\n",
    "> If all of the inputs are categorical, the model is known as anova\n",
    "\n",
    "This connection has been mentioned multiple times, but I'm never been able to exactly understand what that is. In any case, this seems to suggest that anova is nothing fancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Robust linear regression\n",
    "\n",
    "### pp. 226\n",
    "\n",
    "Table 7.1 gives a good summary of many possibilities in regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Ridge Regression\n",
    "\n",
    "### pp. 229\n",
    "\n",
    "In 7.5.2, the author introduces some implementation tricks for Ridge regression. While I roughly speaking buy the first one reducing ridge regression to least squares, I think the idea of the second one ($D \\gg N$) is more often formulated using kernel trick, instead of doing SVD (which may be correct or not, given number of errors in the book). A good description of this kernel trick is available [here](./chapter07/Kernel-Ridge.pdf) (original link <http://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf>), or Section 6.1 of PRML.\n",
    "\n",
    "Well, I derived Eq. (7.44), and it's correct. Essentially, you first start from Eq. (5) in [here](./chapter07/Kernel-Ridge.pdf) (notice that $\\Phi$ in that note and $X$ here are transpose of each other), and then using $A=(BB^{-1})A=A(CC^{-1})$ and then use $(AB)^{-1} = B^{-1} A^{-1}$ multiple times. Notice that $Z^T Z$ is actually $S^2$.\n",
    "\n",
    "Also, the setences below Eq. (7.44) are useful, pointing out that this trick is nothing more than doing transformation on $Z=XV$, do the fitting in that space, and then convert back.\n",
    "\n",
    "### pp. 231\n",
    "\n",
    "Figure 7.9 is also discussed in Figure 3.15 of PRML. I think here MLAPP makes the point (connecting ridge regression with shrinkage from a Bayesian perspective) more clearly. I think the original derivation of this part in (Hastie et al. 2009, pp. 66) assumes that $D < N$, and rank of covariance matrix is $D$. Otherwise, Eq. (7.50) won't hold.\n",
    "\n",
    "Key idea of 7.5.3 is that, while least square is just projecting $y$ in the column space of feature matrix $x$, ridge regression modifies it a little, shrinking parameters more in directions where we have less data supporting (small $\\sigma^2$), and less in directions with more data (big $\\sigma^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Bayesian linear regression\n",
    "\n",
    "### pp. 239\n",
    "\n",
    "7.6.6.3 talks about a case where Bayesian credible interval and Frequentist confidence interval are the same. But this may not be true in general."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
