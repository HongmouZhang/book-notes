{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bayesian concept learning\n",
    "\n",
    "I like this example very much. This really demonstrates the power of Bayesian paradigm, not just some coin flipping toy example.\n",
    "\n",
    "### pp. 70\n",
    "Eq. (3.5) is wrong in its format, but I think we know what it means.\n",
    "\n",
    "### pp. 74\n",
    "before 3.2.5. Full Bayesian approach and plug-in approach (MAP) are quite different when sample size is small.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Naive Bayes classifiers\n",
    "\n",
    "### pp. 91\n",
    "\n",
    "Dirichlet Compound Model (DCM) looks like a very clever way to model burstiness. It looks like the final result of sequential updating posterior, each time making already seen words more likely to be seen. Check <https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution#Dirichlet-multinomial_as_a_compound_distribution>. Basically, this is an extension to multinomial distribution, where balls are drawn in the normal scheme; here, it's drawn in [PÃ³lya urn scheme](https://en.wikipedia.org/wiki/P%C3%B3lya_urn_model). In some level, it's analogous to hypergeometric distribution, or multinomial distribution, except that these distributions have different models on drawing.\n",
    "\n",
    "The problem with DCM is that it's no longer naive Bayes, so it's more difficult to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
