{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 The exponential family\n",
    "\n",
    "For exponential family in general, check my spectial note on it. Here only some bugs for this section are listed.\n",
    "\n",
    "### Section 9.2.4, pp. 288\n",
    "\n",
    "Here some notation is not defined before. In particular I think $g(\\eta(\\theta))$ is $1/Z(\\theta)$ (or $\\exp{(-A(\\eta(\\theta)))}$ using only notation in Eq. (9.5)), and the derivation here assumes $\\eta(\\theta) = \\theta$ (canonical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Generalized linear models\n",
    "\n",
    "This part is essentially similar to the corresponding part of Jordan's PGM book (chapter 8 of `PGM_Jordan` in this repo). I think here the author is more rigorous, noticing that here he only presents GLM in scalar case, and where sufficient statistic is the variable itself, and that such presentation omits multinonimal distribution.\n",
    "\n",
    "The paragraph below Eq. (9.94) gives some general strategy when dealing with non-canonical link functions (Fisher scoring method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Probit regression\n",
    "\n",
    "I personally don't see the advantage of probit over logistic, just based on the author's presentation, as they only differ in terms of gaussian vs. multinomial, in some sense. But I think multinomial probit might be useful for modeling correlated binary variables, as in multinominal distribution, there's no notion of correlation.\n",
    "\n",
    "The reason that in 9.4.2 and 9.4.4, we can use standard Gaussian, or zero mean Gaussian with correlation matrix (a covariance matrix where all components have unit variance), is that, for variance part, we can always make it to be unit, by scaling $w$. For the mean part, we can always add one term in $w$, and add a $1$ in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Learning to rank\n",
    "\n",
    "In the paragraph above 9.7.1, the author says that the motivation for methods presented here is that, apart from standard measure between document and query, we'd like to incorporate other measures as well.\n",
    "\n",
    "In 9.7.3, in the sentences below Eq. (9.121), the author gives an interpretation of each term in it, using a three term example. Indeed, that interpretation (first term being probability A being ranked first, second term being B ranked second given A ranked first, etc.; or more generally, given a set of indices $I$, $\\sum_{i \\in I} s_i$ being probability these terms are ranked earlier than others) seems to be valid, since the probability of these terms in $I$ ranked first, by summing all possibilities in Eq. (9.121), seems to be equal to $\\sum_{i \\in I} s_i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
