{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0\n",
    "\n",
    "### pp. 229 Many types of regularization, and goal of regularization\n",
    "\n",
    "> Sometimes these constraints and penalties are designed to encode specific kinds of prior knowledge. Other times, these constraints and penalties are designed to express a generic preference for a simpler model class in order to promote generalization. Sometimes penalties and constraints are necessary to make an underdetermined problem determined. Other forms of regularization, known as ensemble methods, combine multiple hypotheses that explain the training data.\n",
    "\n",
    "> we focused on three situations, where the model family being trained either (1) excluded the true data generating process—corresponding to underfitting and inducing bias, or (2) matched the true data generating process, or (3) included the generating process but also many other possible generating processes ... The goal of regularization is to take a model from the third regime into the second regime.\n",
    "\n",
    "### deep learning models are always wrong, and regularization makes it less wrong, in the sense of reduced test error, instead of finding the so-called correct ones.\n",
    "\n",
    "> In practice, an overly complex model family does not necessarily include the target function or the true data generating process, or even a close approximation of either. ... Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe. To some extent, we are always trying to fit a square peg (the data generating process) into a round hole (our model family).\n",
    "\n",
    "I think this would be all Bayesian modeling comparison methods in PRML are fundamentally wrong, which assumes that we can find the correct model family.\n",
    "\n",
    "> What this means is that controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios,we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Parameter Norm Penalties\n",
    "\n",
    "### pp. 230 two reasons why not regularizing bias\n",
    "\n",
    "First, bias is easier to estimate; second, it doesn't make sense to assume bias should be small.\n",
    "\n",
    "> Fitting the weight well requires observing both variables in a variety of conditions. Each bias controls only a single variable. This means that we do not induce too much variance by leaving the biases unregularized. Also, regularizing the bias parameters can introduce a significant amount of underfitting.\n",
    "\n",
    "### pp. 232-233 interpretation of ridge regression\n",
    "\n",
    "> We see that the effect of weight decay is to rescale $w^*$ along the axes defined bythe eigenvectors of $H$.\n",
    "\n",
    "> Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact.\n",
    "\n",
    "I think similar observations are made in other books as well (Figure 7.9 of MLAPP, and Figure 3.15 of PRML), although maybe from different perspectives. I like the practical approach here.\n",
    "\n",
    "Some notes on Figure 7.1:\n",
    "\n",
    "the hessian has small eigenvalue on $x$ axis, because the value of quadratic function changes slowly, and large eigenvalue on $y$ axis, because it changes fast (thus having more dense contour). This drawing is based on what I learned in convex optimization: that a regularizer is equivalent as a constraint that the regularizer's norm is smaller than a certain number. Thus we have this tangent like behavior. It’s also (more directly) due to KKT condition.\n",
    "\n",
    "### pp. 236 why is L1 more sparse\n",
    "\n",
    "> In comparison to $L^2$ regularization, $L^1$ regularization results in a solution that is more sparse. ... If $w^∗$ was nonzero, then $\\tilde{w}_i$ remains nonzero. This demonstrates that $L^2$ regularization does not cause the parameters to become sparse, while $L^1$ regularization may do so for large enough $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Norm Penalties as Constrained Optimization\n",
    "\n",
    "### pp. 238 connection between constrained optimization and penalty.\n",
    "\n",
    "> In principle, one can solve for $k$, but the relationship between $k$ and $\\alpha$ depends on the form of $J$. While we do not know the exact size of the constraint region, we can control it roughly by increasing or decreasing $\\alpha$ in order to grow or shrink the constraint region.\n",
    "\n",
    "I forgot whether this relationship only depends on form of J, or also training data (well depending on data is fine...)\n",
    "\n",
    "> Larger $\\alpha$ will result in a smaller constraint region. Smaller $\\alpha$ will result in a larger constraint region.\n",
    "\n",
    "In addition, not sure if this larger->smaller, smaller->larger claim can be proved. But it makes sense.\n",
    "\n",
    "Later on in this page, they mention scenarios you want constrained optimization, instead of unconstrained. First is that sometimes we have an idea of the size of the constrained region, and another is that norm penalty can lead to some undesirable local minima. I think this second point should be due to nonconvexity of neural networks. if neural network is convex, then two methods (penalty and constrained) should be equivalent.\n",
    "\n",
    "Last, it can be helpful for some optimization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Regularization and Under-Constrained Problems\n",
    "\n",
    "I think here underdetermined means multiple solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Dataset Augmentation\n",
    "\n",
    "### pp. 240 augmentation may not be possible for some tasks\n",
    "\n",
    "> For example, it is difficult to generate new fake data for a density estimation task unless we have already solved the density estimation problem.\n",
    "\n",
    "### pp. 241 not easy to determine whether comparisons are fair when comparing two methods\n",
    "\n",
    "> Sometimes deciding whether an experiment has been properly controlled requires subjective judgment. For example, machine learning algorithms that inject noise into the input are performing a form of dataset augmentation. Usually, operations that are generally applicable (such as adding Gaussian noise to the input) are considered part of the machine learning algorithm, while operations that are specific to one application domain (such as randomly cropping an image) are considered to be separate pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Noise Robustness\n",
    "\n",
    "### pp. 242 noise can be more powerful than weight decay\n",
    "\n",
    "> In the general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units.\n",
    "\n",
    "### pp. 242 you can even add noise to weight\n",
    "\n",
    "> The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.\n",
    "\n",
    "later on, the author gives an example where weight noise is considered as some traditional regularization. But seems that the derivation is approximate.\n",
    "\n",
    "### pp. 243 you can also add noise to label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Semi-Supervised Learning\n",
    "\n",
    "### pp. 244 combining supervised loss and unsupervised loss\n",
    "\n",
    "> By controlling how much of the generative criterion is included in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Multi-Task Learning\n",
    "\n",
    "### pp. 244 why this works.\n",
    "\n",
    "> In the same way that additional training examples put more pressure on the parameters of the model towards values that generalize well, when part of a model is shared across tasks, that part of the model is more constrained towards good values (assuming the sharing is justified), often yielding better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Early Stopping\n",
    "\n",
    "### pp. 247 early stopping is used widely and can be seen as hyperparamter selection.\n",
    "\n",
    "> This strategy is known as early stopping. It is probably the most commonly used form of regularization in deep learning.\n",
    "\n",
    "> One way to think of early stopping is as a very efficient hyperparameter selection algorithm. In this view, the number of training steps is just another hyperparameter.\n",
    "\n",
    "### pp. 249-252 how early stopping works as weight decay.\n",
    "\n",
    "some technical notes\n",
    "\n",
    "* Eq. (7.39): we should iterate on $Q^T(w^{(k)} - w^*)$.\n",
    "* above Eq. (7.40), the condition that $|1-\\epsilon \\lambda_i| < 1$: I don't think this is needed to make 7.40 valid. Although I guess to make the logarithm approximation below to work you need this.\n",
    "* Eq. (7.42): 7.42 from 7.41 should be from Woodbury matrix formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Sparse Representations\n",
    "\n",
    "### pp. 255 there are other sparsity other than L1.\n",
    "\n",
    "> Of course, the $L_1$ penalty is only one choice of penaltythat can result in a sparse representation.\n",
    "\n",
    "### pp. 256 sparsity is everywhere.\n",
    "\n",
    "Essentially any model that has hidden units can be made sparse. Throughout this book, we will see many examples of sparsity regularization used in a variety of contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Bagging and Other Ensemble Methods\n",
    "\n",
    "In the analysis of pp. 256, it assumes that mean of error $\\epsilon$ is 0.\n",
    "\n",
    "### pp. 256 ensemble should always work.\n",
    "\n",
    "> In other words, on average, the ensemble will perform at least as well as any of its members, and if the members make independent errors, the ensemble will perform significantly better than its members.\n",
    "\n",
    "### pp. 257-258 essentials of bagging, and source of randomness in bagging\n",
    "\n",
    "> Bagging is a method that allows the same kind of model, training algorithm and objective function to be reused several times. ... Differences in random initialization, random selection of minibatches, differences in hyperparameters, or different outcomes of non-deterministic implementations of neural networks are often enough to cause different members of the ensemble to make partially independent errors.\n",
    "\n",
    "### pp. 258 benchmarking doesn't use bagging, as it always improve performance.\n",
    "\n",
    "> For this reason, benchmark comparisons are usually made using a single model.\n",
    "\n",
    "### pp. 258, ensemble methods can also make overfitting.\n",
    "\n",
    "> Not all techniques for constructing ensembles are designed to make the ensemble more regularized than the individual models. For example, a technique called boosting (Freund and Schapire, 1996b,a) constructs an ensemble with higher capacity than the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 Dropout\n",
    "\n",
    "### pp. 259 typical parameters of Dropout; essence of dropout, and dropout and bagging.\n",
    "\n",
    "> Typically, an input unit is included with probability 0.8 and a hidden unit is included with probability 0.5.\n",
    "\n",
    "> This parameter sharing makes it possible to represent an exponential number of models with a tractable amount of memory.\n",
    "\n",
    "> Beyond these, dropout follows the bagging algorithm. For example, the training set encountered by each sub-network is indeed a subset of the original training set sampled with replacement.\n",
    "\n",
    "I think this is only true when you have multiple batches. Not true with only one batch.\n",
    "\n",
    "### pp. 260 poential dead path in Dropout for small networks.\n",
    "\n",
    "> This problem becomes insignificant for networks with wider layers, where the probability of dropping all possible paths from inputs to outputs becomes smaller.\n",
    "\n",
    "### pp. 263-264 theories for dropout in deep networks don't exist; but it performs well.\n",
    "\n",
    "> We call this approach the weight scaling inference rule. There is not yet any theoretical argument for the accuracy of this approximate inference rule in deep nonlinear networks, but empirically it performs very well.\n",
    "\n",
    "> However, the weight scaling rule is only an approximation for deep models that have nonlinearities. Though the approximation has not been theoretically characterized, it often works well, empirically.\n",
    "\n",
    "### pp. 265-266 best approximate inference can be problem dependent; dropout requires bigger model to offset regularization; dropout is equivalent to weight decay for some simple models but not for deep models.\n",
    "\n",
    "> It appears that the optimal choice of inference approximation is problem-dependent.\n",
    "\n",
    "> Because dropout is a regularization technique, it reduces the effective capacity of a model. To offset this effect, we must increase the size of the model.\n",
    "\n",
    "> For deep models, dropout is not equivalent to weight decay.\n",
    "\n",
    "### pp. 266 randomness is not necessary nor sufficient for the success of dropout\n",
    "\n",
    "> The stochasticity used while training with dropout is not necessary for the approach’s success. It is just a means of approximating the sum over all sub-models.\n",
    "\n",
    "> Just as stochasticity is not necessary to achieve the regularizing effect of dropout, it is also not sufficient.\n",
    "\n",
    "essentially this says that the effect of dropout depends on the optimization procedure used to optimize your model. the masks generated by dropout doesn't themselves have regularization effects.\n",
    "\n",
    "### pp. 267-268 dropout is more effective for hidden units; dropout uses superior multiplicative noise; batch normalization as using additive and multiplicative noise.\n",
    "\n",
    "> Traditional noise injection techniques that add unstructured noise at the input are not able to randomly erase the information about a nose from an image of a face unless the magnitude of the noise is so great ... Destroying extracted features rather than original values allows the destruction process to make use of all of the knowledge about the input distribution that the model has acquired so far.\n",
    "\n",
    "In Hao Wang's occlusion paper (\"Learning Robust Object Recognition Using Composed Scenes from Generative Models\"), essentially we are using input-level dropout. I think in this case, indeed noise level is very high to completely remove the information. But still it doesn't work. Maybe our noise is too structured?\n",
    "\n",
    "> Multiplicative noise does not allow such a pathological solution to the noise robustness problem.\n",
    "\n",
    "> Another deep learning algorithm, batch normalization, reparametrizes the model in a way that introduces both additive and multiplicative noise on the hidden units at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.13 Adversarial Training\n",
    "\n",
    "### pp. 268 linearity can cause adversarial examples.\n",
    "\n",
    "> one of the primary causes of these adversarial examples is excessive linearity.\n",
    "\n",
    "Also check CS231n (<http://vision.stanford.edu/teaching/cs231n/slides/winter1516_lecture9.pdf>) there they have a good example about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier\n",
    "\n",
    "### pp. 270 tricky part about tmanifold distance.\n",
    "\n",
    "> nearest-neighbor distance between points $x_1$ and $x_2$ the distance between the manifolds $M_1$ and $M_2$ to which they respectively belong.\n",
    "\n",
    "but how to define $M_1$ and $M_2$, and how we know what $M$ is associated with $x$?\n",
    "\n",
    "### pp. 270 tangent prop equation (7.67)\n",
    "\n",
    "there are multiple tangent vectors indexed using $i$, since a $N$-d space, a $N-1$-d plane has $N-1$ (independent) tangents.\n",
    "\n",
    "### pp. 271 problem of tangent prop\n",
    "\n",
    "> First, it only regularizes the model to resist infinitesimal perturbation. ... Second, the infinitesimal approach poses difficulties for models based on rectified linear units.\n",
    "\n",
    "I think the second point is to say that it's probably useless to regularize ReLU's gradient in terms of infinitesimal perturbation, as the gradient would probably not change at all. Instead, we must use some non local transformation for regularization purpose, as in data augmentation.\n",
    "\n",
    "### pp. 271 4 related regularization methods\n",
    "\n",
    "> Just as dataset augmentation is the non-infinitesimal version of tangent propagation, adversarial training is the non-infinitesimal version of double backprop.\n",
    "\n",
    "difference between dataset augmentation and adversarial training is in whether regularizing specified directions or all directions.\n",
    "\n",
    "### pp. 271 manifold tangent classifier can help to learn tangent vectors. Check Chapter 14.\n",
    "\n",
    "> The manifold tangent classifier (Rifai et al., 2011c), eliminates the need to know the tangent vectors a priori."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
