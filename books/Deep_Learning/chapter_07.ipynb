{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0\n",
    "\n",
    "### pp. 229 Many types of regularization, and goal of regularization\n",
    "\n",
    "> Sometimes these constraints and penalties are designed to encode specific kinds of prior knowledge. Other times, these constraints and penalties are designed to express a generic preference for a simpler model class in order to promote generalization. Sometimes penalties and constraints are necessary to make an underdetermined problem determined. Other forms of regularization, known as ensemble methods, combine multiple hypotheses that explain the training data.\n",
    "\n",
    "> we focused on three situations, where the model family being trained either (1) excluded the true data generating process—corresponding to underfitting and inducing bias, or (2) matched the true data generating process, or (3) included the generating process but also many other possible generating processes ... The goal of regularization is to take a model from the third regime into the second regime.\n",
    "\n",
    "### deep learning models are always wrong, and regularization makes it less wrong, in the sense of reduced test error, instead of finding the so-called correct ones.\n",
    "\n",
    "> In practice, an overly complex model family does not necessarily include the target function or the true data generating process, or even a close approximation of either. ... Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe. To some extent, we are always trying to fit a square peg (the data generating process) into a round hole (our model family).\n",
    "\n",
    "I think this would be all Bayesian modeling comparison methods in PRML are fundamentally wrong, which assumes that we can find the correct model family.\n",
    "\n",
    "> What this means is that controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios,we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Parameter Norm Penalties\n",
    "\n",
    "### pp. 230 two reasons why not regularizing bias\n",
    "\n",
    "First, bias is easier to estimate; second, it doesn't make sense to assume bias should be small.\n",
    "\n",
    "> Fitting the weight well requires observing both variables in a variety of conditions. Each bias controls only a single variable. This means that we do not induce too much variance by leaving the biases unregularized. Also, regularizing the bias parameters can introduce a significant amount of underfitting.\n",
    "\n",
    "### pp. 232-233 interpretation of ridge regression\n",
    "\n",
    "> We see that the effect of weight decay is to rescale $w^*$ along the axes defined bythe eigenvectors of $H$.\n",
    "\n",
    "> Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact.\n",
    "\n",
    "I think similar observations are made in other books as well (Figure 7.9 of MLAPP, and Figure 3.15 of PRML), although maybe from different perspectives. I like the practical approach here.\n",
    "\n",
    "Some notes on Figure 7.1:\n",
    "\n",
    "the hessian has small eigenvalue on $x$ axis, because the value of quadratic function changes slowly, and large eigenvalue on $y$ axis, because it changes fast (thus having more dense contour). This drawing is based on what I learned in convex optimization: that a regularizer is equivalent as a constraint that the regularizer's norm is smaller than a certain number. Thus we have this tangent like behavior. It’s also (more directly) due to KKT condition.\n",
    "\n",
    "### pp. 236 why is L1 more sparse\n",
    "\n",
    "> In comparison to $L^2$ regularization, $L^1$ regularization results in a solution that is more sparse. ... If $w^∗$ was nonzero, then $\\tilde{w}_i$ remains nonzero. This demonstrates that $L^2$ regularization does not cause the parameters to become sparse, while $L^1$ regularization may do so for large enough $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Norm Penalties as Constrained Optimization\n",
    "\n",
    "### pp. 238 connection between constrained optimization and penalty.\n",
    "\n",
    "> In principle, one can solve for $k$, but the relationship between $k$ and $\\alpha$ depends on the form of $J$. While we do not know the exact size of the constraint region, we can control it roughly by increasing or decreasing $\\alpha$ in order to grow or shrink the constraint region.\n",
    "\n",
    "I forgot whether this relationship only depends on form of J, or also training data (well depending on data is fine...)\n",
    "\n",
    "> Larger $\\alpha$ will result in a smaller constraint region. Smaller $\\alpha$ will result in a larger constraint region.\n",
    "\n",
    "In addition, not sure if this larger->smaller, smaller->larger claim can be proved. But it makes sense.\n",
    "\n",
    "Later on in this page, they mention scenarios you want constrained optimization, instead of unconstrained. First is that sometimes we have an idea of the size of the constrained region, and another is that norm penalty can lead to some undesirable local minima. I think this second point should be due to nonconvexity of neural networks. if neural network is convex, then two methods (penalty and constrained) should be equivalent.\n",
    "\n",
    "Last, it can be helpful for some optimization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Regularization and Under-Constrained Problems\n",
    "\n",
    "I think here underdetermined means multiple solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Dataset Augmentation\n",
    "\n",
    "### pp. 240 augmentation may not be possible for some tasks\n",
    "\n",
    "> For example, it is difficult to generate new fake data for a density estimation task unless we have already solved the density estimation problem.\n",
    "\n",
    "### pp. 241 not easy to determine whether comparisons are fair when comparing two methods\n",
    "\n",
    "> Sometimes deciding whether an experiment has been properly controlled requires subjective judgment. For example, machine learning algorithms that inject noise into the input are performing a form of dataset augmentation. Usually, operations that are generally applicable (such as adding Gaussian noise to the input) are considered part of the machine learning algorithm, while operations that are specific to one application domain (such as randomly cropping an image) are considered to be separate pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Noise Robustness\n",
    "\n",
    "### pp. 242 noise can be more powerful than weight decay\n",
    "\n",
    "> In the general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units.\n",
    "\n",
    "### pp. 242 you can even add noise to weight\n",
    "\n",
    "> The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.\n",
    "\n",
    "later on, the author gives an example where weight noise is considered as some traditional regularization. But seems that the derivation is approximate.\n",
    "\n",
    "### pp. 243 you can also add noise to label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Semi-Supervised Learning\n",
    "\n",
    "### pp. 244 combining supervised loss and unsupervised loss\n",
    "\n",
    "> By controlling how much of the generative criterion is included in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Multi-Task Learning\n",
    "\n",
    "### pp. 244 why this works.\n",
    "\n",
    "> In the same way that additional training examples put more pressure on the parameters of the model towards values that generalize well, when part of a model is shared across tasks, that part of the model is more constrained towards good values (assuming the sharing is justified), often yielding better generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
