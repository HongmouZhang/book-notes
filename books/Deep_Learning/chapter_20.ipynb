{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.1 Boltzmann Machines\n",
    "\n",
    "### pp. 655 the Boltzmann machine is a universal approximator of discrete probability distributions\n",
    "\n",
    "> Just as the addition of hidden units to convert logistic regression into an MLP results in the MLP being a universal approximator of functions, a Boltzmann machine with hidden units is no longer limited to modeling linear relationships between variables. Instead, the Boltzmann machine becomes a universal approximator of probability mass functions over discrete variables (Le Roux and Bengio, 2008).\n",
    "\n",
    "### pp. 656 Hebbian learning and BM\n",
    "\n",
    "> In particular, in the positive phase, two units that frequently activate together have their connection strengthened. This is an example of a Hebbian learning rule (Hebb, 1949) often summarized with the mnemonic “fire together, wire together.” Hebbian learning rules are among the oldest hypothesized explanations for learning in biological systems and remain relevant today (Giudice et al., 2009).\n",
    "\n",
    "### pp. 656 some papers about the biological plausibility of backpropagation.\n",
    "\n",
    "> Proposals for biologically plausible implementations (and approximations) of back-propagation have been made (Hinton, 2007a; Bengio, 2015) but remain to be validated, and Bengio (2015) links back-propagation of gradients to inference in energy-based models similar to the Boltzmann machine (but with continuous latent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.2 Restricted Boltzmann machines\n",
    "\n",
    "### pp. 658 partition function of RBM is hard to compute.\n",
    "\n",
    "> In the case of restricted Boltzmann machines, Long and Servedio (2010) formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P(v) is also intractable to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.3 Deep Belief Networks\n",
    "\n",
    "### pp. 660 people still remember DBN.\n",
    "\n",
    "> Today, deep belief networks have mostly fallen out of favor and are rarely used, even compared to other unsupervised or generative learning algorithms, but they are still deservedly recognized for their important role in deep learning history.\n",
    "\n",
    "### pp. 661 exponential family DBN\n",
    "\n",
    "> Generalizations to other exponential family visible units are straightforward, at least in theory. A DBN with only one hidden layer is just an RBM.\n",
    "\n",
    "Honglak Lee has a thesis chapter on expoenential family DBN.\n",
    "\n",
    "### pp. 662 DBN finetuning is ad-hoc, and does not follow any variational principle\n",
    "\n",
    "> This specific choice of MLP is somewhat arbitrary, compared to many of the inference equations in chapter 19 that are derived from first principles. This MLP is a heuristic choice that seems to work well in practice and is used consistently in the literature. Many approximate inference techniques are motivated by their ability to find a maximally tight variational lower bound on the log-likelihood under some set of constraints. ... In particular, the MLP ignores many important interactions in the DBN graphical model. The MLP propagates information upward from the visible units to the deepest hidden units, but does not propagate any information downward or sideways. The DBN graphical model has explaining away interactions between all of the hidden units within the same layer as well as top-down interactions between layers.\n",
    "\n",
    "Essentially, MLP gives an approximation to hidden layer's true posterior. But this approximation is not tight w.r.t. DBN's true posterior in any way, unlike mean field or some other variational principles.\n",
    "\n",
    "### pp. 662 to evaluate quality of generation.\n",
    "\n",
    "> While the log-likelihood of a DBN is intractable, it may be approximated with AIS (Salakhutdinov and Murray, 2008). This permits evaluating its quality as a generative model.\n",
    "\n",
    "### pp. 662 confusion about term \"DBN\"\n",
    "\n",
    "> The term “deep belief network” is commonly used incorrectly to refer to any kind of deep neural network, even networks without latent variable semantics. The term “deep belief network” should refer specifically to models with undirected connections in the deepest layer and directed connections pointing downward between all other pairs of consecutive layers.\n",
    ">\n",
    "> The term “deep belief network” may also cause some confusion because the term “belief network” is sometimes used to refer to purely directed models, while deep belief networks contain an undirected layer. Deep belief networks also share the acronym DBN with dynamic Bayesian networks (Dean and Kanazawa, 1989), which are Bayesian networks for representing Markov chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.4 Deep Boltzmann Machines\n",
    "\n",
    "MP-DBM sounds like something \n",
    "\n",
    "### pp. 666 DBM used for neuroscience\n",
    "\n",
    "> Because of this property, DBMs have been used as computational models of real neuroscientific phenomena (Series et al., 2010; Reichert et al., 2011).\n",
    "\n",
    "### pp. 668-669 Learning DBM using Persistent CD (also called Stochastic ML)\n",
    "\n",
    "Persistent CD and SML are the same thing, as mentioned in Chapter 18.\n",
    "\n",
    "### pp. 671 tricks in initializing DBM\n",
    "\n",
    "> Greedy layer-wise pretraining of a DBM differs from greedy layer-wise pretraining of a DBN.\n",
    "\n",
    "This chapter talks about things like dividing weights in half for middle RBMs, and doubling for top and bottom RBMs, etc. Those details are in the original DBM paper, and just some hack to make DBM work with RBM initialization.\n",
    "\n",
    "### pp. 673 alternative ways to train DBM\n",
    "\n",
    "there are two methods. Centered DBM and multi-prediction DBM.\n",
    "\n",
    "Centerd DBM is just a way to make learning from scratch easier. But seems that, based on book author's descriptions, it's better than training DBM from scratch naively, but worse than layerwise pretraining in the original DBM paper.\n",
    "\n",
    "Multi-prediction DBM (check Figure 20.5) essentially trains DBM for some other task. In original DBM, people first train a generative model, and then use weights for classification, etc. There's some difference between training and testing tasks.\n",
    "\n",
    "However, in MP DBM, they are the same (pp.674). Check first advantage. Although I don't understand why this helps classification. Maybe check the original paper for detail.\n",
    "\n",
    "> Back-propagation through the inference graph has two main advantages. First, it trains the model as it is really used—with approximate inference. This means that approximate inference, for example, to fill in missing inputs, or to perform classification despite the presence of missing inputs, is more accurate in the MP-DBM than in the original DBM.\n",
    "\n",
    "### pp. 676 MP-DBM and Dropout\n",
    "\n",
    "> The MP-DBM has some connections to dropout. Dropout shares the same pa- rameters among many different computational graphs, with the difference between each graph being whether it includes or excludes each unit. The MP-DBM also shares parameters across many computational graphs. In the case of the MP-DBM, the difference between the graphs is whether each input unit is observed or not. When a unit is not observed, the MP-DBM does not delete it entirely as dropout does. Instead, the MP-DBM treats it as a latent variable to be inferred. One could imagine applying dropout to the MP-DBM by additionally removing some units rather than making them latent.\n",
    "\n",
    "dropout drops units, and MP-DBM makes them hidden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
