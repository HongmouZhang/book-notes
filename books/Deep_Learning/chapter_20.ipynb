{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.1 Boltzmann Machines\n",
    "\n",
    "### pp. 655 the Boltzmann machine is a universal approximator of discrete probability distributions\n",
    "\n",
    "> Just as the addition of hidden units to convert logistic regression into an MLP results in the MLP being a universal approximator of functions, a Boltzmann machine with hidden units is no longer limited to modeling linear relationships between variables. Instead, the Boltzmann machine becomes a universal approximator of probability mass functions over discrete variables (Le Roux and Bengio, 2008).\n",
    "\n",
    "### pp. 656 Hebbian learning and BM\n",
    "\n",
    "> In particular, in the positive phase, two units that frequently activate together have their connection strengthened. This is an example of a Hebbian learning rule (Hebb, 1949) often summarized with the mnemonic “fire together, wire together.” Hebbian learning rules are among the oldest hypothesized explanations for learning in biological systems and remain relevant today (Giudice et al., 2009).\n",
    "\n",
    "### pp. 656 some papers about the biological plausibility of backpropagation.\n",
    "\n",
    "> Proposals for biologically plausible implementations (and approximations) of back-propagation have been made (Hinton, 2007a; Bengio, 2015) but remain to be validated, and Bengio (2015) links back-propagation of gradients to inference in energy-based models similar to the Boltzmann machine (but with continuous latent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.2 Restricted Boltzmann machines\n",
    "\n",
    "### pp. 658 partition function of RBM is hard to compute.\n",
    "\n",
    "> In the case of restricted Boltzmann machines, Long and Servedio (2010) formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P(v) is also intractable to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.3 Deep Belief Networks\n",
    "\n",
    "### pp. 660 people still remember DBN.\n",
    "\n",
    "> Today, deep belief networks have mostly fallen out of favor and are rarely used, even compared to other unsupervised or generative learning algorithms, but they are still deservedly recognized for their important role in deep learning history.\n",
    "\n",
    "### pp. 661 exponential family DBN\n",
    "\n",
    "> Generalizations to other exponential family visible units are straightforward, at least in theory. A DBN with only one hidden layer is just an RBM.\n",
    "\n",
    "Honglak Lee has a thesis chapter on expoenential family DBN.\n",
    "\n",
    "### pp. 662 DBN finetuning is ad-hoc, and does not follow any variational principle\n",
    "\n",
    "> This specific choice of MLP is somewhat arbitrary, compared to many of the inference equations in chapter 19 that are derived from first principles. This MLP is a heuristic choice that seems to work well in practice and is used consistently in the literature. Many approximate inference techniques are motivated by their ability to find a maximally tight variational lower bound on the log-likelihood under some set of constraints. ... In particular, the MLP ignores many important interactions in the DBN graphical model. The MLP propagates information upward from the visible units to the deepest hidden units, but does not propagate any information downward or sideways. The DBN graphical model has explaining away interactions between all of the hidden units within the same layer as well as top-down interactions between layers.\n",
    "\n",
    "Essentially, MLP gives an approximation to hidden layer's true posterior. But this approximation is not tight w.r.t. DBN's true posterior in any way, unlike mean field or some other variational principles.\n",
    "\n",
    "### pp. 662 to evaluate quality of generation.\n",
    "\n",
    "> While the log-likelihood of a DBN is intractable, it may be approximated with AIS (Salakhutdinov and Murray, 2008). This permits evaluating its quality as a generative model.\n",
    "\n",
    "### pp. 662 confusion about term \"DBN\"\n",
    "\n",
    "> The term “deep belief network” is commonly used incorrectly to refer to any kind of deep neural network, even networks without latent variable semantics. The term “deep belief network” should refer specifically to models with undirected connections in the deepest layer and directed connections pointing downward between all other pairs of consecutive layers.\n",
    ">\n",
    "> The term “deep belief network” may also cause some confusion because the term “belief network” is sometimes used to refer to purely directed models, while deep belief networks contain an undirected layer. Deep belief networks also share the acronym DBN with dynamic Bayesian networks (Dean and Kanazawa, 1989), which are Bayesian networks for representing Markov chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.4 Deep Boltzmann Machines\n",
    "\n",
    "MP-DBM sounds like something \n",
    "\n",
    "### pp. 666 DBM used for neuroscience\n",
    "\n",
    "> Because of this property, DBMs have been used as computational models of real neuroscientific phenomena (Series et al., 2010; Reichert et al., 2011).\n",
    "\n",
    "### pp. 668-669 Learning DBM using Persistent CD (also called Stochastic ML)\n",
    "\n",
    "Persistent CD and SML are the same thing, as mentioned in Chapter 18.\n",
    "\n",
    "### pp. 671 tricks in initializing DBM\n",
    "\n",
    "> Greedy layer-wise pretraining of a DBM differs from greedy layer-wise pretraining of a DBN.\n",
    "\n",
    "This chapter talks about things like dividing weights in half for middle RBMs, and doubling for top and bottom RBMs, etc. Those details are in the original DBM paper, and just some hack to make DBM work with RBM initialization.\n",
    "\n",
    "### pp. 673 alternative ways to train DBM\n",
    "\n",
    "there are two methods. Centered DBM and multi-prediction DBM.\n",
    "\n",
    "Centerd DBM is just a way to make learning from scratch easier. But seems that, based on book author's descriptions, it's better than training DBM from scratch naively, but worse than layerwise pretraining in the original DBM paper.\n",
    "\n",
    "Multi-prediction DBM (check Figure 20.5) essentially trains DBM for some other task. In original DBM, people first train a generative model, and then use weights for classification, etc. There's some difference between training and testing tasks.\n",
    "\n",
    "However, in MP DBM, they are the same (pp.674). Check first advantage. Although I don't understand why this helps classification. Maybe check the original paper for detail.\n",
    "\n",
    "> Back-propagation through the inference graph has two main advantages. First, it trains the model as it is really used—with approximate inference. This means that approximate inference, for example, to fill in missing inputs, or to perform classification despite the presence of missing inputs, is more accurate in the MP-DBM than in the original DBM.\n",
    "\n",
    "### pp. 676 MP-DBM and Dropout\n",
    "\n",
    "> The MP-DBM has some connections to dropout. Dropout shares the same pa- rameters among many different computational graphs, with the difference between each graph being whether it includes or excludes each unit. The MP-DBM also shares parameters across many computational graphs. In the case of the MP-DBM, the difference between the graphs is whether each input unit is observed or not. When a unit is not observed, the MP-DBM does not delete it entirely as dropout does. Instead, the MP-DBM treats it as a latent variable to be inferred. One could imagine applying dropout to the MP-DBM by additionally removing some units rather than making them latent.\n",
    "\n",
    "dropout drops units, and MP-DBM makes them hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.5 Boltzmann Machines for Real-Valued Data\n",
    "\n",
    "Here two types of extensions to BM for real-valued data are introduced. First (20.5.1), the vanilla Gaussian RBM. Notice that there is some freedom in how to parameterize it. At least from what I know, Honglak Lee's thesis and Hinton RBM training guide don't parametrize GBM the same.\n",
    "\n",
    "Later on (20.5.2-end) they introduced ways to parameterize covariance matrix (or precision matrix) into GBM, as well. Vanilla one uses diagonal matrix, which may not be sufficient to represent natural image.\n",
    "\n",
    "Well there's some confusion. In those sparse coding and ICA work, as well as these non-trivial covariance matrix GBMs, whitened training images are used. However, this white only means that any two pixels are not correlated over all images. It doesn't say whether given the hidden variables, the conditional distribution of visible variables have non-trivial covariance. (well, image data self can't answer this, as it doesn't specify a generative model). \n",
    "\n",
    "I think compared to sparse coding, which assumes trivial (identity time some number) covariance matrix, we can compare these fancy GBM to sparse coding as LSTM to RNN. In both cases, while the latter can emulate the first one through very careful training, in practice it's much better to simply incorporate additional assumptions and structures into the model as in the former.\n",
    "\n",
    "There are three ways to extend GBM. mcRBM, mPoT, and ssRBM. Seems that first two are similar, and difficult to train (using hybrid Monte Carlo). ssRBM is easier to train, being sparse (due to spike units), and has significantly different covariance structure (see pp. 682), and this means ssRBM is good for sparse representation.\n",
    "\n",
    "> Comparing the ssRBM to the mcRBM and the mPoT models, the ssRBM parametrizes the conditional covariance of the observation in a significantly different way. The mcRBM and mPoT both model the covariance structure ... In contrast,\n",
    "the ssRBM specifies the conditional covariance of the observations using the hidden\n",
    "spike activations $h_i = 1$ to pinch the precision matrix along the direction specified\n",
    "by the corresponding weight vector.\n",
    ">\n",
    "> In the overcomplete setting, sparse activations with the ssRBM parametrization permit significant variance only in the selected directions of the sparsely activated $h_i$. In the mcRBM or mPoT models, an overcomplete representation would mean that to capture variation in a particular direction in the observation space requires removing potentially all constraints with positive projection in that direction. This would suggest that these models are less well suited to the overcomplete setting.\n",
    "\n",
    "Essentially, in mcRBM and mPoT models, we need many hidden units to be activated for the conditional distribution of observation to be precise (high precision), except in a few directinos. However, in ssRBM, we need few hidden units. This is because in the mcRBM and mPoT, ON hidden units **increase** precision, and in ssRBM, ON hidden units **subtract** precision. This means that if we assume that the conditional distribution should be precise (low variance) except in a few directions, then ssRBM fits this assumption the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.6 Convolutional Boltzmann Machines\n",
    "\n",
    "Main issue of conv Deep Boltzmann Machine is the introduction of pooling. There should be a way to sample the hidden layer given the pooling layer above, for Gibbs sampling during learning as well as inference. If we naively use the usual definition of max pooling, then given a pooling unit's value (which is a deterministic function of the hidden units it pools from), there are simply too many possibilities of the below hidden units giving such pooling unit. For example, a pooling unit pooling from 3x3 binary hidden units. If this pooling unit is zero, then definitely hidden units must be zero; however, if it's 1, then we should sample the hidden units with $2^9-1$ different configurations, since they all yield pooling unit being 1. This can be difficult to sample (well we could also use a Gibbs sampling for this; but that's probably too expensive; in addition, given pooling unit, all the hidden units are not independent of each other; thus in general we can't avoid this exponential cost in sampling; maybe it's not that bad, but still it can be messy; otherwise, Honglak Lee's probabilistic max pooling won't be invented in the first place).\n",
    "\n",
    "Honglak Lee's probabilistic max pooling essentially restricts the possibility of possible configurations of hidden units for a 3x3 units to be 3x3 + 1 (N x N + 1 for pooling size of N). This allows more efficient sampling.\n",
    "\n",
    "### pp. 684 convolutional DBN is difficult to make work in practice.\n",
    "\n",
    "> While intellectually appealing, this model is challenging to make work in practice, and usually does not perform as well as a classifier as traditional convolutional networks trained with supervised learning.\n",
    "\n",
    "### pp. 683-685 some limitations of CNN + DBN.\n",
    "\n",
    "1. pooling in DBN doesn't allow overlapping. I think this is because then the distribution would be super complicated to sample.\n",
    "2. pooling in DBN can't change kernel size easily without affecting model performance a lot. I think as long as you don't change kernel size, it's fine.\n",
    "3. Boundary pixel problem. Here the authors mention that at boundary, either hidden units won't fire as often, or visible units can't be modeled well. This is partially because we only allow symmetric weights between hidden and visible units, and this may not be a problem, if we model encoding (visible to hidden) and decoding (hidden to visible) separately using two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.7 Boltzmann Machines for Structured or Sequential Outputs.\n",
    "\n",
    "This chapter introduces some papers of RBM for modeling sequences, such as generating chracter movement in graphics (I remember Hinton has such a paper, and it's said that model training takes days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.8 Other Boltzmann Machines\n",
    "\n",
    "Here they introduce less popular variant of BMs, such as factored 3 way RBM, gated RBMs, etc. Maybe worth reading if one likes to design some fancy BM variants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
