{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.1 Boltzmann Machines\n",
    "\n",
    "### pp. 655 the Boltzmann machine is a universal approximator of discrete probability distributions\n",
    "\n",
    "> Just as the addition of hidden units to convert logistic regression into an MLP results in the MLP being a universal approximator of functions, a Boltzmann machine with hidden units is no longer limited to modeling linear relationships between variables. Instead, the Boltzmann machine becomes a universal approximator of probability mass functions over discrete variables (Le Roux and Bengio, 2008).\n",
    "\n",
    "### pp. 656 Hebbian learning and BM\n",
    "\n",
    "> In particular, in the positive phase, two units that frequently activate together have their connection strengthened. This is an example of a Hebbian learning rule (Hebb, 1949) often summarized with the mnemonic “fire together, wire together.” Hebbian learning rules are among the oldest hypothesized explanations for learning in biological systems and remain relevant today (Giudice et al., 2009).\n",
    "\n",
    "### pp. 656 some papers about the biological plausibility of backpropagation.\n",
    "\n",
    "> Proposals for biologically plausible implementations (and approximations) of back-propagation have been made (Hinton, 2007a; Bengio, 2015) but remain to be validated, and Bengio (2015) links back-propagation of gradients to inference in energy-based models similar to the Boltzmann machine (but with continuous latent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.2 Restricted Boltzmann machines\n",
    "\n",
    "### pp. 658 partition function of RBM is hard to compute.\n",
    "\n",
    "> In the case of restricted Boltzmann machines, Long and Servedio (2010) formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P(v) is also intractable to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.3 Deep Belief Networks\n",
    "\n",
    "### pp. 660 people still remember DBN.\n",
    "\n",
    "> Today, deep belief networks have mostly fallen out of favor and are rarely used, even compared to other unsupervised or generative learning algorithms, but they are still deservedly recognized for their important role in deep learning history.\n",
    "\n",
    "### pp. 661 exponential family DBN\n",
    "\n",
    "> Generalizations to other exponential family visible units are straightforward, at least in theory. A DBN with only one hidden layer is just an RBM.\n",
    "\n",
    "Honglak Lee has a thesis chapter on expoenential family DBN.\n",
    "\n",
    "### pp. 662 DBN finetuning is ad-hoc, and does not follow any variational principle\n",
    "\n",
    "> This specific choice of MLP is somewhat arbitrary, compared to many of the inference equations in chapter 19 that are derived from first principles. This MLP is a heuristic choice that seems to work well in practice and is used consistently in the literature. Many approximate inference techniques are motivated by their ability to find a maximally tight variational lower bound on the log-likelihood under some set of constraints. ... In particular, the MLP ignores many important interactions in the DBN graphical model. The MLP propagates information upward from the visible units to the deepest hidden units, but does not propagate any information downward or sideways. The DBN graphical model has explaining away interactions between all of the hidden units within the same layer as well as top-down interactions between layers.\n",
    "\n",
    "Essentially, MLP gives an approximation to hidden layer's true posterior. But this approximation is not tight w.r.t. DBN's true posterior in any way, unlike mean field or some other variational principles.\n",
    "\n",
    "### pp. 662 to evaluate quality of generation.\n",
    "\n",
    "> While the log-likelihood of a DBN is intractable, it may be approximated with AIS (Salakhutdinov and Murray, 2008). This permits evaluating its quality as a generative model.\n",
    "\n",
    "### pp. 662 confusion about term \"DBN\"\n",
    "\n",
    "> The term “deep belief network” is commonly used incorrectly to refer to any kind of deep neural network, even networks without latent variable semantics. The term “deep belief network” should refer specifically to models with undirected connections in the deepest layer and directed connections pointing downward between all other pairs of consecutive layers.\n",
    ">\n",
    "> The term “deep belief network” may also cause some confusion because the term “belief network” is sometimes used to refer to purely directed models, while deep belief networks contain an undirected layer. Deep belief networks also share the acronym DBN with dynamic Bayesian networks (Dean and Kanazawa, 1989), which are Bayesian networks for representing Markov chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.4 Deep Boltzmann Machines\n",
    "\n",
    "MP-DBM sounds like something \n",
    "\n",
    "### pp. 666 DBM used for neuroscience\n",
    "\n",
    "> Because of this property, DBMs have been used as computational models of real neuroscientific phenomena (Series et al., 2010; Reichert et al., 2011).\n",
    "\n",
    "### pp. 668-669 Learning DBM using Persistent CD (also called Stochastic ML)\n",
    "\n",
    "Persistent CD and SML are the same thing, as mentioned in Chapter 18.\n",
    "\n",
    "### pp. 671 tricks in initializing DBM\n",
    "\n",
    "> Greedy layer-wise pretraining of a DBM differs from greedy layer-wise pretraining of a DBN.\n",
    "\n",
    "This chapter talks about things like dividing weights in half for middle RBMs, and doubling for top and bottom RBMs, etc. Those details are in the original DBM paper, and just some hack to make DBM work with RBM initialization.\n",
    "\n",
    "### pp. 673 alternative ways to train DBM\n",
    "\n",
    "there are two methods. Centered DBM and multi-prediction DBM.\n",
    "\n",
    "Centerd DBM is just a way to make learning from scratch easier. But seems that, based on book author's descriptions, it's better than training DBM from scratch naively, but worse than layerwise pretraining in the original DBM paper.\n",
    "\n",
    "Multi-prediction DBM (check Figure 20.5) essentially trains DBM for some other task. In original DBM, people first train a generative model, and then use weights for classification, etc. There's some difference between training and testing tasks.\n",
    "\n",
    "However, in MP DBM, they are the same (pp.674). Check first advantage. Although I don't understand why this helps classification. Maybe check the original paper for detail.\n",
    "\n",
    "> Back-propagation through the inference graph has two main advantages. First, it trains the model as it is really used—with approximate inference. This means that approximate inference, for example, to fill in missing inputs, or to perform classification despite the presence of missing inputs, is more accurate in the MP-DBM than in the original DBM.\n",
    "\n",
    "### pp. 676 MP-DBM and Dropout\n",
    "\n",
    "> The MP-DBM has some connections to dropout. Dropout shares the same pa- rameters among many different computational graphs, with the difference between each graph being whether it includes or excludes each unit. The MP-DBM also shares parameters across many computational graphs. In the case of the MP-DBM, the difference between the graphs is whether each input unit is observed or not. When a unit is not observed, the MP-DBM does not delete it entirely as dropout does. Instead, the MP-DBM treats it as a latent variable to be inferred. One could imagine applying dropout to the MP-DBM by additionally removing some units rather than making them latent.\n",
    "\n",
    "dropout drops units, and MP-DBM makes them hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.5 Boltzmann Machines for Real-Valued Data\n",
    "\n",
    "Here two types of extensions to BM for real-valued data are introduced. First (20.5.1), the vanilla Gaussian RBM. Notice that there is some freedom in how to parameterize it. At least from what I know, Honglak Lee's thesis and Hinton RBM training guide don't parametrize GBM the same.\n",
    "\n",
    "Later on (20.5.2-end) they introduced ways to parameterize covariance matrix (or precision matrix) into GBM, as well. Vanilla one uses diagonal matrix, which may not be sufficient to represent natural image.\n",
    "\n",
    "Well there's some confusion. In those sparse coding and ICA work, as well as these non-trivial covariance matrix GBMs, whitened training images are used. However, this white only means that any two pixels are not correlated over all images. It doesn't say whether given the hidden variables, the conditional distribution of visible variables have non-trivial covariance. (well, image data self can't answer this, as it doesn't specify a generative model). \n",
    "\n",
    "I think compared to sparse coding, which assumes trivial (identity time some number) covariance matrix, we can compare these fancy GBM to sparse coding as LSTM to RNN. In both cases, while the latter can emulate the first one through very careful training, in practice it's much better to simply incorporate additional assumptions and structures into the model as in the former.\n",
    "\n",
    "There are three ways to extend GBM. mcRBM, mPoT, and ssRBM. Seems that first two are similar, and difficult to train (using hybrid Monte Carlo). ssRBM is easier to train, being sparse (due to spike units), and has significantly different covariance structure (see pp. 682), and this means ssRBM is good for sparse representation.\n",
    "\n",
    "> Comparing the ssRBM to the mcRBM and the mPoT models, the ssRBM parametrizes the conditional covariance of the observation in a significantly different way. The mcRBM and mPoT both model the covariance structure ... In contrast,\n",
    "the ssRBM specifies the conditional covariance of the observations using the hidden\n",
    "spike activations $h_i = 1$ to pinch the precision matrix along the direction specified\n",
    "by the corresponding weight vector.\n",
    ">\n",
    "> In the overcomplete setting, sparse activations with the ssRBM parametrization permit significant variance only in the selected directions of the sparsely activated $h_i$. In the mcRBM or mPoT models, an overcomplete representation would mean that to capture variation in a particular direction in the observation space requires removing potentially all constraints with positive projection in that direction. This would suggest that these models are less well suited to the overcomplete setting.\n",
    "\n",
    "Essentially, in mcRBM and mPoT models, we need many hidden units to be activated for the conditional distribution of observation to be precise (high precision), except in a few directinos. However, in ssRBM, we need few hidden units. This is because in the mcRBM and mPoT, ON hidden units **increase** precision, and in ssRBM, ON hidden units **subtract** precision. This means that if we assume that the conditional distribution should be precise (low variance) except in a few directions, then ssRBM fits this assumption the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.6 Convolutional Boltzmann Machines\n",
    "\n",
    "Main issue of conv Deep Boltzmann Machine is the introduction of pooling. There should be a way to sample the hidden layer given the pooling layer above, for Gibbs sampling during learning as well as inference. If we naively use the usual definition of max pooling, then given a pooling unit's value (which is a deterministic function of the hidden units it pools from), there are simply too many possibilities of the below hidden units giving such pooling unit. For example, a pooling unit pooling from 3x3 binary hidden units. If this pooling unit is zero, then definitely hidden units must be zero; however, if it's 1, then we should sample the hidden units with $2^9-1$ different configurations, since they all yield pooling unit being 1. This can be difficult to sample (well we could also use a Gibbs sampling for this; but that's probably too expensive; in addition, given pooling unit, all the hidden units are not independent of each other; thus in general we can't avoid this exponential cost in sampling; maybe it's not that bad, but still it can be messy; otherwise, Honglak Lee's probabilistic max pooling won't be invented in the first place).\n",
    "\n",
    "Honglak Lee's probabilistic max pooling essentially restricts the possibility of possible configurations of hidden units for a 3x3 units to be 3x3 + 1 (N x N + 1 for pooling size of N). This allows more efficient sampling.\n",
    "\n",
    "### pp. 684 convolutional DBN is difficult to make work in practice.\n",
    "\n",
    "> While intellectually appealing, this model is challenging to make work in practice, and usually does not perform as well as a classifier as traditional convolutional networks trained with supervised learning.\n",
    "\n",
    "### pp. 683-685 some limitations of CNN + DBN.\n",
    "\n",
    "1. pooling in DBN doesn't allow overlapping. I think this is because then the distribution would be super complicated to sample.\n",
    "2. pooling in DBN can't change kernel size easily without affecting model performance a lot. I think as long as you don't change kernel size, it's fine.\n",
    "3. Boundary pixel problem. Here the authors mention that at boundary, either hidden units won't fire as often, or visible units can't be modeled well. This is partially because we only allow symmetric weights between hidden and visible units, and this may not be a problem, if we model encoding (visible to hidden) and decoding (hidden to visible) separately using two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.7 Boltzmann Machines for Structured or Sequential Outputs.\n",
    "\n",
    "This chapter introduces some papers of RBM for modeling sequences, such as generating chracter movement in graphics (I remember Hinton has such a paper, and it's said that model training takes days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 20.8 Other Boltzmann Machines\n",
    "\n",
    "Here they introduce less popular variant of BMs, such as factored 3 way RBM, gated RBMs, etc. Maybe worth reading if one likes to design some fancy BM variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.9 Back-Propagation through Random Operations\n",
    "\n",
    "this part introduces reparameterization trick. While on its own it looks just like a trick. In practice, it makes possible to take the derivative of some expectation operator (say $\\int f(x) p_{\\theta}(x) \\mathrm{d}x$) w.r.t. to the parameters of the distribution $p_\\theta(x)$ by taking the sampled average of the gradient of $f(x) p_{\\theta}(x)$. Otherwise, after sampling, you can't obtain a gradient w.r.t. $\\theta$, or the estimator has too high variance in practice (see Kingma and Welling's VAE paper <https://arxiv.org/abs/1312.6114>).\n",
    "\n",
    "### 20.9.1 Back-Propagating through Discrete Stochastic Operations\n",
    "\n",
    "Here, the derivative is not smooth, almost zero everywhere, and having discontinuities. The idea is to compute the gradient w.r.t. to some expectation. I think in practice this is not bad, as in many cases, what we really want to optimize is indeed the expectation, not some particular sampling results.\n",
    "\n",
    "One very useful trick is given below Eq. (20.62), $\\frac{\\partial \\log p(y)}{\\partial w} = \\frac{1}{p(y)} \\frac{\\partial p(y)}{\\partial w}$. This is also used in the VAE paper mentioned above.\n",
    "\n",
    "As in practice, the expectation can't be computed, and sampling has to be used. Eq. (20.59)-(20.62) gives the way to compute this gradient by sampling. I think this is same as the \"navie\" estimator mentioned in VAE paper. As mentioned by both VAE paper and here, it has very high variance, and some remedies are proposed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.10 Directed Generative Nets\n",
    "\n",
    "### 20.10.1 Sigmoid Belief Nets\n",
    "\n",
    "#### pp. 693 inference in Sigmoid is intractable\n",
    "\n",
    "> While generating a sample of the visible units is very efficient in a sigmoid belief network, most other operations are not. Inference over the hidden units given the visible units is intractable. Mean field inference is also intractable because the variational lower bound involves taking expectations of cliques that encompass entire layers. This problem has remained difficult enough to restrict the popularity of directed discrete networks.\n",
    "\n",
    "I think the reason that Mean-Field doesn't work is because SBN usually has multiple layers, and while we can do mean-field with one hidden layer, as in Boltzmann machine like models, such as Binary Sparse Coding as shown in the book, it can't deal with case of multiple (directed) layers. For multiple undirected layers, we can convert them into a big layer to perform mean-field, as in Deep Boltzmann Machine.\n",
    "\n",
    "For more, check Lecture 13-3 **Learning Sigmoid Belief Networks** for Hinton's Coursera course. I think while it's still possible to simply sample from the higher layer prior and combine this with some sampled likelihood to get posteriors, it may not work in practice, due to time taken to make MCMC converge. This naive sampling method may also fail to find those actually high probability posteriors. Conceptually, I think it's the same problem as why we can't simply sample some examples from a mixture distribution to perform learning. See Carl Doersch's [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) (v2), pp. 7.\n",
    "\n",
    "### 20.10.2 Differentiable Generator Nets\n",
    "\n",
    "This section gives general strategy for VAE, GAN, and maybe other methods. That is, transforming a simple distribution into a more complicated distribution.\n",
    "\n",
    "While in theory we can apply Eq. (20.73), in practice it's difficult to evaluate, such as determinant, and inverse, etc. Therefore, other approaches are needed.\n",
    "\n",
    "#### pp. 695 two ways of using generator nets. samples directly or parameters of samples.\n",
    "\n",
    "> The two different approaches to formulating generator nets—emitting the parameters of a conditional distribution versus directly emitting samples—have complementary strengths and weaknesses. ... When the generator net provides samples directly, it is capable of generating only continuous data  ... The advantage to direct sampling is that we are no longer forced to use conditional distributions whose form can be easily written down and algebraically manipulated by a human designer.\n",
    "\n",
    "#### pp. 696 we have strong enough generator nets, as least when latent variables are given.\n",
    "\n",
    "> Dosovitskiy et al. (2015) studied a simplified problem, where the correspondence between z and x is given. ... This suggests that contemporary differentiable generator networks have sufficient model capacity to be good generative models, and that contemporary optimization algorithms have the ability to fit them. The difficulty lies in determining how to train generator networks when the value of z for each x is not fixed and known ahead of each time.\n",
    "\n",
    "The next step is to learn the latent variables directly. All the later sections in 20.10 are about this.\n",
    "\n",
    "### 20.10.3 Variational Autoencoders\n",
    "\n",
    "For derivation of Eq. (20.76) to Eq. (20.78), check Eq. (19.1) to (19.7).\n",
    "\n",
    "#### pp. 697 two ways of interpreting the variational lower bound.\n",
    "\n",
    "For Eq. (20.76)\n",
    "\n",
    ">  More generally, this entropy term encourages the variational posterior to place high probability mass on many z values that could have generated x, rather than collapsing to a single point estimate of the most likely value.\n",
    "\n",
    "For Eq. (20.77)\n",
    "\n",
    ">  first term as the reconstruction log-likelihood found in other autoencoders. The second term tries to make the approximate posterior distribution ... and the model prior ... approach each other.\n",
    "\n",
    "In the VAE tutorial paper, there's an information theory interpretation.\n",
    "\n",
    "Check my notes on VAE tutorial and VAE paper for a more detailed explanation.\n",
    "\n",
    "#### pp. 698 DRAW\n",
    "\n",
    "DRAW sounds like a VAE, but with more fancy, recurrent decoder, plus some attention mechanism. Some statements here look confusing to me, but they may need paper reading I guess. For example I don't understand \"Generating a sample from a traditional RNN involves only non-deterministic operations at the output space. Variational RNNs also have random variability at the potentially more abstract level captured by the VAE latent variables.\". Actually I'm not sure how RNN generate examples. I think usually, output of RNN is some softmax unit, that is, parameters of some distribution. But can't you insert some randomness in the input of RNN? I think this is similar to latent variables in VAE. Anyway.\n",
    "\n",
    "#### pp. 698 importance weighted autoencoder.\n",
    "\n",
    "Essentially opimizing a tighter bound than that in VAE, and this may allow the lower bound to be more accurate, as in VAE, the difference between lower bound and the true log probability (Check Eq. (19.1)) is the penalty of the KL between estimated (factorized) posterior and true posterior, which can be large. Using the bound in the importance weighted AE paper doesn't have this problem, as long as sample number is big enough.\n",
    "\n",
    "Minor details: the importance weighted AE paper also shows that, for weighted sampling to work, you need to have bounded ratios. See Appendix A of [Importance Weighted Autoencoders](https://arxiv.org/abs/1509.00519) (v4).\n",
    "\n",
    "### 20.10.4 Generative Adversarial Networks\n",
    "\n",
    "The main cost function of GAN is given is given in Eq. (20.80), and I think most designs and papers (see <https://zhuanlan.zhihu.com/p/26491601>, or <https://github.com/hindupuravinash/the-gan-zoo>) around GAN work around the following problems. 1) the form of $v$. 2) variations on Eq. (20.80), in terms of what's mined, what's maxed, etc.\n",
    "\n",
    "There are many problems with learning a GAN. In pp. 701, non-convergence of simultaneous gradient descent is discussed. A more detailed version is given in [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160) (v4). Check Sections 7.2 and 8.2.\n",
    "\n",
    "I think it's claimed that [Wasserstein GAN](https://arxiv.org/abs/1701.07875) solves many problems around original GAN. See also <https://zhuanlan.zhihu.com/p/25071913>.\n",
    "\n",
    "### 20.10.5 Generative Moment Matching Networks\n",
    "\n",
    "Sounds like a differentiable generator net earlier than VAE and GAN, using a loss called maximum mean discrepancy, which essentially compares distributions using moments.\n",
    "\n",
    "One big problem of this approach is that the cost is defined on a batch, and this means single example stochastic gradient descent doesn't work, and mini batch methods may be unstable.\n",
    "\n",
    "### 20.10.6 Convolutional Generative Networks\n",
    "\n",
    "I think here they simply mentioned some issues when using CNN as a generator. CGN is not a model per se.\n",
    "\n",
    "### 20.10.7-20.10.10\n",
    "\n",
    "These are all bell and whistle versions of Linear Auto-Regressive Networks, which is just writing down a multivariate Gaussian using chain-rule. (Check chapter 2 of PRML about partitioned Gaussian, you will know that this is always possible). I don't think they are widely used, based on the years of references talking about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.11 Drawing Samples from Autoencoders\n",
    "\n",
    "20.11.1-20.11.2 just talks about how to sample from denosiing autoencoder, which implicitly defines a probability distribution, by sampling.\n",
    "\n",
    "20.11.3 is just a training trick for such autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.12 Generative Stochastic Networks\n",
    "\n",
    "This method learns a sampling process, which implicitly defines a distribution.\n",
    "\n",
    "20.12.1 sounds like to me that, instead of having steps 1 and 2 in pp. 715, they have K steps, with each step outputting output k, conditioned on first k-1 outputs, plus some hidden variables. So instead of having at most two terms on right part of conditional distribution as in Step 2, they may have many terms? Anyway..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.13 Other Generation Schemes\n",
    "\n",
    "Too fancy... Don't understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.14 Evaluating Generative Models\n",
    "\n",
    "This chapter is worth reading. Main message is that evaluation is difficult and ambiguous.\n",
    "\n",
    "In practice, usually one shows some samples, as well as training data points closest to these samples, showing that the network is not simply remembering data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
