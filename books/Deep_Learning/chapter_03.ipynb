{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12 Technical Details of Continuous Variables\n",
    "\n",
    "### pp. 71 why we do not need mesure theory in Deep Learning\n",
    "\n",
    "You only need to understand several concepts informally.\n",
    "\n",
    "> One of the key contributions of measure theory is to provide a characterization of the set of sets that we can compute the probability of without encountering paradoxes. In this book, we only integrate over sets with relatively simple descriptions, so this aspect of measure theory never becomes a relevant concern.\n",
    ">\n",
    "> it is sufficient to understand the intuition that a set of measure zero occupies no volume in the space we are measuring.\n",
    ">\n",
    "> Some important results in probability theory hold for all discrete values but only hold “almost everywhere” for continuousvalues\n",
    "\n",
    "Later on, change of variable is also mentioned. That is important as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13 Information Theory\n",
    "\n",
    "### pp. 73 rataionale for information theory\n",
    "\n",
    "> We would like to quantify information in a way that formalizes this intuition. Specifically, ... In order to satisfy all three of these properties, we define the self-information of an event $x = x$ to be ... \n",
    "\n",
    "Check 1.6 of PRML (end of pp. 48, and Ex 1.28) for more formal derivation of this.\n",
    "\n",
    "### pp. 74-75 \n",
    "\n",
    "> This asymmetry means that there are important consequences to the choice of whether to use $D_{KL}(P\\|Q)$ or $D_{KL}(Q\\|P)$. See figure 3.6 for more detail.\n",
    "\n",
    "this is also shown in variational inference part for Ruslan's lecture note for DL class (<http://www.cs.cmu.edu/~rsalakhu/10807_2016/Lectures/Lecture_Variational_Inference.pdf>).\n",
    "\n",
    "> Minimizing the cross-entropy with respect to $Q$ is equivalent to minimizing the\n",
    "KL divergence, because $Q$ does not participate in the omitted term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 Structured Probabilistic Models\n",
    "\n",
    "### pp. 78-79 MRF or BN is not inherent to distributions.\n",
    "\n",
    "> Being directed or undirected is not a property of a probability distribution; it is a property of a particular description of a probability distribution, but any probability distribution may be described in both ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
