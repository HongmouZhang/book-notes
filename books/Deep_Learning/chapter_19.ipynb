{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 19.2 Expectation Maximization\n",
    "\n",
    "Here EM is considered from a variational perpsective. It can be considered as a coordinate ascent algorithm, changing $q$ and $\\theta$ in turn to maximize $\\mathcal{L}$.\n",
    "\n",
    "### pp. 635 EM and gradient ascent on latent variable models.\n",
    "\n",
    "> Stochastic gradient ascent on latent variable models can be seen as a special case of the EM algorithm where the M step consists of taking a single gradient step. Other variants of the EM algorithm can make much larger steps. For some model families, the M step can even be performed analytically, jumping all the\n",
    "way to the optimal solution for $\\theta$ given the current q.\n",
    "\n",
    "Here I think he's talking about those latent variable models where the true $\\log p(X)$ cannot be computed, and the $\\mathcal{L}$ must be used (well I guess for most interesting latent variable models you have to make some approximations). However, suppose we can find the optimal $q$ in such models. Then we are actually always optimizing a tight $\\mathcal{L}$. So it's kind of like EM, except that in M step, we don't find the maximium of our current estimate of $\\mathcal{L}$, but only one step following gradient, since we can't maximize it for some problems.\n",
    "\n",
    "The \"other variants\" including THE EM algorithm for Gaussian Mixture Model.\n",
    "\n",
    "When doing M step, the gradient being followed is that of $\\mathcal{L}$, not that of $\\log p(X)$. Therefore, there will be gap. \n",
    "\n",
    "> Specifically, the M-step assumes that the same value of q can be used for all values of $\\theta$. This will introduce a gap between $\\mathcal{L}$ and the true $\\log p(v)$ as the M-step moves further and further away from the value $\\theta^{(0)}$ used in the E-step. Fortunately, the E-step reduces the gap to zero again as we enter the loop for the next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 19.3 MAP Inference and Sparse Coding\n",
    "\n",
    "Sparse coding can be thought as doing variational learning (a \"stochastic gradient ascent\" algorithm as described in the above quote of pp. 635), where the $q$ distribution is heavily limited, as shown in Eq. (19.11). While intuitively this makes sense, mathematically there's problem, since entropy of such $q$ distributions is negative infinite. But that should be some nuisance stuff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 19.4 Variational Inference and Learning\n",
    "\n",
    "### pp. 639 two types of KL\n",
    "\n",
    "> In this sense, we are fitting $q$ to $p$. However, we are doing so with the opposite direction of the KL divergence than we are used to using for fitting an approximation. ... this means that maximum likelihood encourages the model to have high probability everywhere that the data has high probability, while our optimization-based inference procedure encourages $q$ to have low probability everywhere the true posterior has low probability.\n",
    "\n",
    "This is also discussed in PRML.\n",
    "\n",
    "Later this paragraph, the authors aruged why this has to be the case: computational reasons.\n",
    "\n",
    "### pp. 639-645 an example of mean field for binary distribution.\n",
    "\n",
    "This is essentially the mean field equation for boltzmann machine. For a pure derivation of MF on Boltzmann machine, see Example 5.2 of Jordan's *Graphical Models, Exponential Families, and Variational Inference*. However, there the mean field algorithm is derived from a different perspective. But anyway, the resultant update equation Eq (5.17) in the correct one.\n",
    "\n",
    "* at end of pp. 642, they say \"In principle, we could simply run gradient ascent on both v and h and this would make a perfectly acceptable combined inference and training algorithm.\", and then argue against it. However, I find this confusing. First, why run gradient ascent on $v$, which is fixed? Second, they argue they don't want per-example memory. But I think no matter what you do you need that per-sample memory. Anyway, in practice, we do fixed point (per factor) iteration.\n",
    "* Below Eq. (19.45), there's some intuition about the equation, which can used for packaging this method in some biological way.\n",
    "* at pp. 645, they talk about dampening, which can be used to accelarate MF iteration approximately.\n",
    "\n",
    "### pp. 645-650 an example of mean field for continuous distribution\n",
    "\n",
    "this part's derivation is sloppy. Conceptually, we can replace all integral with sum to reason about it. Similar material is there in PRML.\n",
    "\n",
    "pp. 647-648 talks about that the search space of functions is important.\n",
    "\n",
    "> What about the probability distribution function that minimizes the entropy? Why did we not find a second critical point corresponding to the minimum? The reason is that there is no specific function that achieves minimal entropy. ... These distributions are thus invisible to our method of solving for a specific point where the functional derivatives are zero. This is a limitation of the method. Distributions such as the Dirac must be found by other methods, such as guessing the solution and then proving that it is correct.\n",
    "\n",
    "This last comment is amusing.\n",
    "\n",
    "### pp. 650 interaction between learning and inference.\n",
    "\n",
    "essentially, quality of inference algorithm is correlated with quality of learned parameters, and it's difficult to debug how bad your inference algorithm is.\n",
    "\n",
    "> Using approximate inference as part of a learning algorithm affects the learning process, and this in turn affects the accuracy of the inference algorithm.\n",
    "\n",
    "> Specifically, the training algorithm tends to adapt the model in a way that makes the approximating assumptions underlying the approximate inference algorithm become more true.\n",
    "\n",
    "> Computing the true amount of harm imposed on a model by a variational approximation is thus very difficult.\n",
    "\n",
    "> To measure the true amount of harm induced by the variational approximation, we would need to know $\\theta^*$ ... \n",
    "\n",
    "> Such a problem is very difficult to detect, because we can only know for sure that it happened if we have a superior learning algorithm that can find $\\theta^*$ for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.5 Learned Approximate Inference\n",
    "\n",
    "Main problem for such approach is that we need supervision for the inference function (pairs of visible, and hidden variables). In 19.5.1, they mentioned the problem of such approach.\n",
    "\n",
    "Then later in 19.5.1, they have some speculation on the function of wake and sleep in human beings.\n",
    "\n",
    "at bottom of pp. 652, they say that variational autoencoder does not have problem of supervision for inference function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
