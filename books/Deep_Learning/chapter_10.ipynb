{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Recurrent Neural Networks\n",
    "\n",
    "### pp. 382-384 teacher forcing\n",
    "\n",
    "I should say that teacher forcing is not methetically equivalent to not using it, but a speed-up over not using it, since we can decouple many interactions between time steps.\n",
    "\n",
    "In pp. 384, they say \"later used in an **open-loop** mode\". I think it should be close-loop. This is consistent that teacher forcing is not equivalent to not using it.\n",
    "\n",
    "### pp. 385-386 gradient computation.\n",
    "\n",
    "Notice that the correct way to understand these equations is that first treat parameters in different time steps independently, and then use multivariable chain rule (<https://www.math.hmc.edu/calculus/tutorials/multichainrule/>) to determine the gradient on shared parameters.\n",
    "\n",
    "### pp. 387-391 RNN as graphical models\n",
    "\n",
    "this section gives perspective on seeing RNNs as graphical models. Some parts are confusing. For example, in pp. 387, \"when we do feed the actual $y$ values (not their prediction, but the actual observed or generated values)\" I don't understand the difference between generated values and prediction. Anyway, all the later on given examples make sense.\n",
    "\n",
    "pp. 389 says that, essentially, the power of RNN is that, by using hidden units, we obtained a chain-like, shared-parameter model, capable of modeling full connection between all output variables.\n",
    "\n",
    "pp. 390 talks about some detail when really considering RNN as a graphical model, such as sampling. I think here, we always assume input values are given, or no input.\n",
    "\n",
    "### pp. 391-394 modeling context in RNN\n",
    "\n",
    "there are multiple approaches, depending on type of input, and our independence assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Bidirectional RNNs\n",
    "\n",
    "### pp. 395\n",
    "\n",
    "these ideas may be worth trying. But Xiaolong Wang told me they didn't work (as of early 2017)\n",
    "\n",
    "> This idea can be naturally extended to 2-dimensional input, such as images, by having four RNNs, each one going in one of the four directions: up, down, left, right. ... Compared to a convolutional network, RNNs applied to images are typically more expensive but allow for long-range lateral interactions between features in the same feature map (Visin et al., 2015; Kalchbrenner et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Encoder-Decoder Sequence-to-Sequence Architecture\n",
    "\n",
    "Notice that this sequence-to-sequence model is different than that in CTC in Alex Graves' Supervised Sequence Labelling with Recurrent Neural Networks. In that book, we can roughly get a one-to-one mapping between x and y, because y is always shorter than x (we can merge adjacent identical outputs). But here it's talking about generic sequence to sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Recursive Neural Networks\n",
    "\n",
    "Maybe this approach doesn't work too well, as there are not recent papers on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 The Challenge of Long-Term Dependencies\n",
    "\n",
    "Essentially this is the central topic for all later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Echo State Networks\n",
    "\n",
    "Essentially ESNs are clever ways to initialize RNNs with good dynamics. It can be used to initialize RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Leaky Units and Other Strategies for Multiple Time Scales\n",
    "\n",
    "I don't think these work, as LSTM is the first choice for most people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 The Long Short-Term Memory and Other Gated RNNs\n",
    "\n",
    "I think in CS231n (early 2016), the instructor talked about connection between LSTM as residual net. Essentially, such design helps gradient propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.12 Explicit Memory\n",
    "\n",
    "authors say that memory addressing is essentially the same as attention-mechanism. I think these models (including attention) are adding prior structure into vanilla RNN models, just as we add convolution to feedforward models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
