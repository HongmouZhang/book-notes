{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 14.0\n",
    "\n",
    "### pp. 502 alternative, more biological ways to train autoencoders\n",
    "\n",
    "> Unlike general feedforward networks, autoencoders may also be trained using recirculation (Hinton and McClelland, 1988), a learning algorithm based on comparing the activations of the network on the original input to the activations on the reconstructed input. Recirculation is regarded as more biologically plausible than back-propagation, but is rarely used for machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 14.1 Undercomplete Autoencoders\n",
    "\n",
    "### pp. 503 equivalence between PCA and linear autoencoder.\n",
    "\n",
    "> When the decoder is linear and $L$ is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA. In this case, an autoencoder trained to perform the copying task has learned the principal subspace of the training data as a side-effect.\n",
    "\n",
    "Let me give a proof of this. Suppose your original data is $D$-dimensional. Then a linear autoencoder with $K$ hidden units has the following equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{x} &= C(Ax + b) + d, A \\in \\mathbb{R}^{K \\times D}, b \\in \\mathbb{R}^{K \\times 1}, C \\in \\mathbb{R}^{D \\times K}, d \\in \\mathbb{R}^{D \\times 1} \\\\\n",
    " & = C(Ax) + (Cb+d)\n",
    "\\end{align}\n",
    "\n",
    "No matter how you learn your 4 (matrix) variables, still, it lives under a $K$-dimensional affine space (because with bias $Cb+d$). Then check pp. 563 of PRML, Eq. (12.10). Based on that derivation, you will know that, $K$-dimensional affine space can't do better than PCA. Thus, your autoencoder can't do better than PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 14.2 Regularized Autoencoders\n",
    "\n",
    "### pp. 504 connection between autoencoder and generative models.\n",
    "\n",
    "> In addition to the methods described here which are most naturally interpreted as regularized autoencoders, nearly any generative model with latent variables and equipped with an inference procedure (for computing latent representations given input) may be viewed as a particular form of autoencoder. Two generative modeling approaches that emphasize this connection with autoencoders are the descendants of the Helmholtz machine\n",
    "\n",
    "### pp. 505 can't formulate sparsity penalty in sparse autoencoder as prior\n",
    "\n",
    "> Unlike other regularizers such as weight decay, there is not a straightforward Bayesian interpretation to this regularizer.\n",
    "\n",
    "### pp. 506 connection between sparse coding and sparse autoencoder\n",
    "\n",
    "> We can think of the autoencoder as approximating this sum with a point estimate for just one highly likely value for h. This is similar to the sparse coding generative model (section 13.4), but with $h$ being the output of the parametric encoder rather than the result of an optimization that infers the most likely $h$.\n",
    "\n",
    "### pp. 508 alternative models to denoising autoencoder\n",
    "\n",
    "> An autoencoder regularized in this way is called a contractive autoencoder or CAE. This approach has theoretical connections to denoising autoencoders, manifold learning and probabilistic modeling. The CAE is described in more detail in section 14.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Representational Power, Layer Size and Depth\n",
    "\n",
    "### pp. 509 advantage of deep encoder.\n",
    "\n",
    "while a shallow (1 hidden layer) autoencoder could approximate any function, and definitely identity (for reconstruction), our purpose of training autoencoder is to learn useful hidden codes. In this aspect, it might be better to use deep autoencoder (say two hidden layers), so that the second hidden layer (which would be the encoded code), can be anything we like, using universal approximation theorem for neural network.\n",
    "\n",
    "Learning hidden code we want is definitely in conflict with reconstruction. But I think the intuition is that, with more layers, and more learning capacity, both goals of reconstruction and learning specific hidden codes can be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Stochastic Encoders and Decoders\n",
    "\n",
    "this section shows that we can instead of predicting $h$ given $x$ for encoding, and reverse for decoding, we can instead predict the parameters of the distribution $p(h\\mid x)$ and vice versa. Suppose that the encoder and decoder and indeed conditional probabilities for a single joint distribution $p(x,h)$, then we can use encoder and decoder via MCMC methods (say Gibbs sampling) to learn parameters. While I'm not sure how you can know that there's a unique $p(h,x)$ satisying the encoder and decoder, assuming that there exists some $p(h,x)$, I assume that in most ML settings, the $p(h,x)$ should be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 Denoising Autoencoders\n",
    "\n",
    "### pp. 513 score matching\n",
    "\n",
    "\n",
    "score matching essentially means the learned distribution should have same values as the source distribution, in terms of some function (here it's using gradient of log probability) at every point in the input space. Not sure how to use this in practice, because in reality we only have a discrete sampling of the source distribution, and gradient would be difficult to compute.\n",
    "\n",
    "Figure 14.4 shows that, when assuming the data distortion process is Gaussian, then the gradient of the reconstruction error, in the denoising autoencoder, gives the score (gradient of log probability of data), up to multiplicative some constant (which should be related to variance of the data distortion process; the additive constant relevant to normalization is ignored). I think this derivation only works when there's only one original $x$ able to produce the distorted one. Anyway, it's not super strict, and just informal proof that maybe we can learn autoencoder by score matching.\n",
    "\n",
    "### pp. 513 linking RBM and autoencoder\n",
    "\n",
    "There are some paragraphs discussing connections between the two, sometimes under subtle conditions. In any case, autoencoder can be thought as some implicit generative model.\n",
    "\n",
    "### pp. 513-514 score matching shows that Autoencoder can be linked to generative models\n",
    "\n",
    "> For continuous-valued x, the denoising criterion with Gaussian corruption and reconstruction distribution yields an estimator of the score that is applicable to general encoder and decoder parametrizations (Alain and Bengio, 2013). This means a generic encoder-decoder architecture may be made to estimate the score ... \n",
    "\n",
    "By \"a generic encoder-decoder architecture\", they mean that, any denoising autoencoder with Gaussian corruption and square loss, its gradient of loss can be thought as estimating some score. Well I don't see why this needs a proof, it's so obvious to me...\n",
    "\n",
    "Well what needs a proof is whether that gradient of loss is really the score of some distribution (having that form $\\hat{x}-\\tilde{x}$ doesn't mean there exists a data distribution whose score is exactly that), above section 14.4.5.1, authors say there's some work on that.\n",
    "\n",
    "It seems to me that, score matching can be thought as a theoretic tool to show that autoencoder can be thought as learning as generative models. Check Chapter 20 for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 Learning Manifolds with Autoencoders\n",
    "\n",
    "### pp. 515-516 Autoencoder learns the manifold\n",
    "\n",
    "> Some machine learning algorithms exploit this idea only insofar as that they learn a function that behaves correctly on the manifold but may have unusual behavior if given an input that is off the manifold. Autoencoders take this idea further and aim to learn the structure of the manifold.\n",
    "\n",
    "Here, by \"learn the structure of the manifold\", I think this refers to the decoder. Essentially, we can sample some random hidden variable, and compute reconstruction, to trace out the manifold learned by the autoencoder. Not every machine learning algorithm allows to do this examination directly.\n",
    "\n",
    "> An important characterization of a manifold is the set of its tangent planes. At a point x on a d-dimensional manifold, the tangent plane is given by d basis vectors that span the local directions of variation allowed on the manifold. As illustrated in figure 14.6, these local directions specify how one can change x infinitesimally while staying on the manifold.\n",
    "\n",
    "I think the statement \"on a d-dimensional manifold, the tangent plane is given by d basis vectors\" is only totally true on the relative interior of this manifold (for relative interior, check Boyd's convex optimization book for a definition). Consider a square, 2D sheet in a 3D space. Clearly, this sheet is a 2D manifold in a 3D space. At each point on the interior that is not on the boundary of the sheet, we can move around the sheet, by specifying two directions on the sheet, and their coefficients. But on boundary, say the top left corner, our freedom is somehow constrained. At least, not all combinations of two local directions gives us legal steps to stay on the manifold. I believe in theory we can construct more tricky examples to further limit our freedom for non-interior points. Anyway, in general, on d-dim manifold, there are d tangent planes at each point is overall correct.\n",
    "\n",
    "### pp. 516 rationale of denoise autoencoder: let the hidden code ignore irrelevant directions, by forcing noised example (with irrelevant direction) getting pushed back to example on the manifold\n",
    "\n",
    "> If the data generating distribution concentrates near a low-dimensional manifold, this yields representations that implicitly capture a local coordinate system for this manifold: only the variations tangent to the manifold around x need to correspond to changes in h = f(x). Hence the encoder learns a mapping from the input space x to a representation space, a mapping that is only sensitive to changes along the manifold directions, but that is insensitive to changes orthogonal to the manifold.\n",
    "\n",
    "### pp. 516-518 manifold is usually demonstrated by local coordinates of data points, also called embeddings.\n",
    "\n",
    "> What is most commonly learned to characterize a manifold is a representation of the data points on (or near) the manifold. Such a representation for a particular example is also called its embedding.\n",
    "\n",
    "### pp. 519 neighbor-based manifold learning would fail, due to complex, nonlocal variations in the manifold for many types of real world data.\n",
    "\n",
    "> A global coordinate system can then be obtained through an optimization or solving a linear system. Figure 14.9 illustrates how a manifold can be tiled by a large number of locally linear Gaussian-like patches (or “pancakes,” because the Gaussians are flat in the tangent directions).\n",
    "> \n",
    "> However, there is a fundamental difficulty with such local non-parametric approaches to manifold learning, raised in Bengio and Monperrus (2005): if the manifolds are not very smooth (they have many peaks and troughs and twists), one may need a very large number of training examples to cover each one of these variations, with no chance to generalize to unseen variations. Indeed, these methods can only generalize the shape of the manifold by interpolating between neighboring examples. Unfortunately, the manifolds involved in AI problems can have very complicated structure that can be difficult to capture from only local interpolation.\n",
    "\n",
    "The arguments are similar to those when arguing why kernel density estimation would fail in Section 5.11.1. Deep Learning introduces additional non-local assumptions that are more valid in reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.7 Contractive Autoencoders\n",
    "\n",
    "Contractive Autoencoder is like infinitesimal version of denoising autoencoder. Remind me discussion at Section 7.14.\n",
    "\n",
    "### pp. 521 CAE encourges flat gradient. In the case of sigmoid, it will saturate units.\n",
    "\n",
    "> When the $\\Omega(h)$ penalty is applied to sigmoidal units, one easy way to shrink the Jacobian is to make the sigmoid units saturate to 0 or 1. This encourages the CAE to encode input points with extreme values of the sigmoid that may be interpreted as a binary code. It also ensures that the CAE will spread its code values throughout most of the hypercube that its sigmoidal hidden units can span.\n",
    "\n",
    "### pp. 522 CAE will learn manifold tangent planes\n",
    "\n",
    "> Reconstruction error alone would encourage the CAE to learn an identity function. The contractive penalty alone would encourage the CAE to learn features that are constant with respect to x. The compromise between these two forces yields an autoencoder whose derivatives $\\frac{\\partial f(x)}{\\partial x}$ are mostly tiny. Only a small number of hidden units, corresponding to a small number of directions in the input, may have significant derivatives.\n",
    "> \n",
    "> Directions x with large Jx rapidly change h, so these are likely to be directions which approximate the tangent planes of the manifold. ...  The directions corresponding to the largest singular values are interpreted as the tangent directions that the contractive autoencoder has learned. Ideally, these tangent directions should correspond to real variations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.8 Predictive Sparse Decomposition\n",
    "\n",
    "> Predictive sparse coding is an example of learned approximate inference.\n",
    "\n",
    "I don't know why PSD is superior in practice. Maybe since it has a feedforward computation mechanism for sparse coding like encoder, so it can be used in an end-to-end framework with back propagation easily. Also, maybe it may allow more overcomplete representation, since in deployment time, it's cheap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
