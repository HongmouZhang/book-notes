{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 How Learning Differs from Pure Optimization\n",
    "\n",
    "### pp. 276 we want to minimize test error, but we only have train set; we want to minimize the model on the relevant loss (say 0-1 loss), but we often can only use a surrogate loss for being possible to do gradient descent.\n",
    "\n",
    "> However, empirical risk minimization is prone to overfitting.\n",
    "\n",
    "but the data generation function is not.\n",
    "\n",
    "> Instead, we must use a slightly different approach, in which the quantity that we actually optimize is even more different from the quantity that we truly want to optimize.\n",
    "\n",
    "### pp. 277 to solve the training set problem, we use early stopping; to solve loss problem, we use surrogate loss.\n",
    "\n",
    "Sometimes surrogate loss can be even better.\n",
    "\n",
    "> In some cases, a surrogate loss function actually results in being able to learn more. ... thus extracting more information from the training data than would have been possible by simply minimizing the average 0-1 loss on the training set.\n",
    "\n",
    "In ML, end of optmization is not necessarily zero gradient. sometimes, early stopping is used. \n",
    "\n",
    "But I feel in general still want to get zero gradient. At least this should be true for most classical ML methods. Maybe here the authors are primarily talking about optimization in deep models. Check Fig 8.1 for interesting observation on training deep model.\n",
    "\n",
    "> A very important difference between optimization in general and optimization as we use it for training algorithms is that training algorithms do not usually halt at a local minimum.\n",
    "\n",
    "\n",
    "### pp. 279 why using minibatch methods\n",
    "\n",
    "> These were traditionally called **minibatch** or **minibatch stochastic** methods and it is now common to simply call them **stochastic** methods.\n",
    "\n",
    "> * Larger batches provide a more accurate estimate of the gradient, but with less than linear returns.\n",
    "> * Small batches can offer a regularizing effect.\n",
    "\n",
    "However, if batch is too small, we also need small learning rate.\n",
    "\n",
    "> Training with such a small batch size might require a small learning rate to maintain stability due to the high variance in the estimate of the gradient.\n",
    "\n",
    "### pp. 279-280 why estimating Hessian is kind of impossible in minibatch setting.\n",
    "\n",
    "> Second-order methods, which use also the Hessian matrix $H$ and compute updates such as $H^{−1}g$, typically require much larger batch sizes like 10,000.\n",
    "\n",
    "In addition, if Hessian has poor conditioning, the result will be worse.\n",
    "\n",
    "### pp. 280 shuffing data points for stochastic methods is important, and asynchronous methods can be useful.\n",
    "\n",
    "> Failing to ever shuffle the examples in any way can seriously reduce the effectiveness of the algorithm.\n",
    "\n",
    "> Many optimization problems in machine learning decompose over examples well enough that we can compute entire separate updates over different examples in parallel. ... Such asynchronous parallel distributed approaches are discussed further in section 12.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Challenges in Neural Network Optimization\n",
    "\n",
    "### pp. 282-283 ill-conditioning, big curvature can slow down training a lot despite large gradient; Newton's method require modification for application in neural networks.\n",
    "\n",
    "> The result is that learning becomes very slow despite the presence of a strong gradient because the learning rate must be shrunk to compensate for even stronger curvature.\n",
    "\n",
    "> Newton’s method requires significant modification before it can be applied to neural networks.\n",
    "\n",
    "### pp. 284-285 multiple ways of non-identifiability in NN; local minima may not be a problem.\n",
    "\n",
    "> In addition to weight space symmetry, many kinds of neural networks have\n",
    "additional causes of non-identifiability.\n",
    "\n",
    "> One can construct small neural networks, even without hidden units, that have local minima with higher cost than the global minimum. ... If local minima with high cost are common, this could pose a serious problem for gradient-based optimization algorithms.\n",
    "\n",
    "But check check latest papers like [Deep Learning without Poor Local Minima](https://arxiv.org/abs/1605.07110). No idea how this is related to those papers here. Maybe different sets of assumptions?\n",
    "\n",
    "> For many years, most practitioners believed that local minima were a common problem plaguing neural network optimization. Today, that does not appear to be the case. ... In high dimensional spaces, it can be very difficult to positively establish that local minima are the problem. Many structures other than local minima also have small gradients.\n",
    "\n",
    "### pp. 285-287 typical behaviors of high-dimensional random functions; same for functions in NN; gradient descent methods seem to be able to handle saddle points well; second order methods fail\n",
    "\n",
    "> Many classes of random functions exhibit the following behavior: in low-dimensional spaces, local minima are common. In higher dimensional spaces, local minima are rare and saddle points are more common. ... In n-dimensional space, it is exponentially unlikely that all n coin tosses will be heads. ... An amazing property of many random functions is that the eigenvalues of the Hessian become more likely to be positive as we reach regions of lower cost. ... This means that local minima are much more likely to have low cost than high cost. Critical points with high cost are far more likely to be saddle points. Critical points with extremely high cost are more likely to be local maxima.\n",
    "\n",
    "> This happens for many classes of random functions. Does it happen for neural networks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders ... have global minima and saddle points but no local minima with higher cost than the global minimum. They observed without proof that these results extend to deeper networks without nonlinearities.\n",
    "\n",
    "I don't think these good news here are inconsistent with statements at pp. 284. Maybe they simply use different model assumptions.\n",
    "\n",
    "> gradient descent empirically seems to be able to escape saddle points in many cases. ... continuous-time gradient descent may be shown analytically to be repelled from, rather than attracted to, a nearby saddle point, but the situation may be different for more realistic uses of gradient descent.\n",
    "\n",
    "> Surprisingly, these visualizations usually do not show many conspicuous obstacles. Prior to the success of\n",
    "stochastic gradient descent for training very large models beginning in roughly 2012, neural net cost function surfaces were generally believed to have much more non-convex structure than is revealed by these projections. \n",
    "\n",
    "> The proliferation of saddle points in high dimensional spaces presumably explains why second-order methods have not succeeded in replacing gradient descent for neural network training. ... Second-order methods remain difficult to scale to large neural networks, but this saddle-free approach holds promise if it could be scaled.\n",
    "\n",
    "### pp. 289-290 cliff structures; more general, we have gradient vanishing and exploding.\n",
    "\n",
    "> Cliff structures are most common in the cost functions for recurrent neural networks, because such models involve a multiplication of many factors, with one factor for each time step. Long temporal sequences thus incur an extreme amount of multiplication.\n",
    "\n",
    "> The vanishing and exploding gradient problem refers to the fact that gradients through such a graph are also scaled according to $\\textrm{diag}{(\\lambda)}^t$ ... The cliff structures described earlier that motivate gradient clipping are an example of the exploding gradient phenomenon.\n",
    "\n",
    "> Recurrent networks use the same matrix W at each time step, but feedforward networks do not, so even very deep feedforward networks can largely avoid the vanishing and exploding gradient problem.\n",
    "\n",
    "### pp. 290-291 we should not worry about inexact gradient much.\n",
    "\n",
    "> Various neural network optimization algorithms are designed to account for imperfections in the gradient estimate. One can also avoid the problem by choosing a surrogate loss function that is easier to approximate than the true loss.\n",
    "\n",
    "### pp. 291-293 local optimization methods can perform poorly; nn functions may not have minima inherently; good initialization can help.\n",
    "\n",
    "> It is possible to overcome all of these problems at a single point and still perform poorly if the direction that results in the most improvement locally does not point toward distant regions of much lower cost.\n",
    "\n",
    "> Figure 8.1 shows that neural networks often do not arrive at a region of small gradient. Indeed, such critical points do not even necessarily exist. For example, the loss function $\\log{p(y\\mid x; \\theta)}$ can lack a global minimum point and instead asymptotically approach some value as the model becomes more confident.\n",
    "\n",
    "I don't agree; I think at these given examples can be solved using some regularization; not sure about more complicated models.\n",
    "\n",
    "> Many existing research directions are aimed at finding good initial points for problems that have difficult global structure, rather than developing algorithms that use non-local moves.\n",
    "\n",
    "> Regardless of which of these problems are most significant, all of them might be avoided if there exists a region of space connected reasonably directly to a solution by a path that local descent can follow, and if we are able to initialize learning\n",
    "within that well-behaved region. This last view suggests research into choosing good initial points for traditional optimization algorithms to use.\n",
    "\n",
    "### pp. 293 optimization theories are useless.\n",
    "\n",
    "> Typically these results have little bearing on the use of neural networks in practice.\n",
    "\n",
    "> Developing more realistic bounds on the performance of optimization algorithms therefore remains an important goal for machine learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Basic Algorithms\n",
    "\n",
    "### pp. 298 it's more important to tune learning rate over time than momentum.\n",
    "\n",
    "> It is less important to adapt $\\alpha$ over time than to shrink $\\epsilon$ over time.\n",
    "\n",
    "### pp. 299 some physics interpretation of momentum. Also check Hinton's Coursera course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Parameter Initialization Strategies\n",
    "\n",
    "I think for CNN with ReLU units, simply use MSRA initialization. See [\n",
    "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Algorithms with Adaptive Learning Rates.\n",
    "\n",
    "Simply check CS231n. <http://vision.stanford.edu/teaching/cs231n/slides/winter1516_lecture6.pdf>. While CS231n recommends Adam, I think in practice a lot of people use simple SGD + momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Approximate Second-Order Methods\n",
    "\n",
    "### pp. 313-315 key idea of conjugate gradient descent is that not undoing what's done before.\n",
    "\n",
    "Check Fig. 8.6 to see why SGD fails. In pp. 314-315, two methods for $\\beta_t$ are given. According to [Wikipedia](https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method), they are same if the cost function is quadratic.\n",
    "\n",
    "> These formulas are equivalent for a quadratic function, but for nonlinear optimization the preferred formula is a matter of heuristics or taste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Optimization Strategies and Meta-Algorithms\n",
    "\n",
    "### pp. 317-321 batch normalization makes learning easier, using reparameterization.\n",
    "\n",
    "### pp. 322 coordinate descent may not work well if there's too much interaction between two variables.\n",
    "\n",
    "Probably that's due to the same cause why first order methods may not work well when having big curvature.\n",
    "\n",
    "### pp. 326 designing models easier to optmize is more important than the reverse.\n",
    "\n",
    "> In practice, it is more important to choose a model family that is easy to\n",
    "optimize than to use a powerful optimization algorithm.\n",
    "\n",
    "> In other words, modern neural nets have been designed so that their local gradient information corresponds reasonably well to moving toward a distant solution.\n",
    "\n",
    "### pp. 327 continuation methods conceptually sound like warming up. but I think they are different in that.\n",
    "\n",
    "1) continutation is more overcoming local minima and 2) warming up in L1 loss function is just a way of speeding up, but no local minima issue, as the function is convex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
