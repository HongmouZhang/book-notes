{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 13.3 Slow Feature Analysis\n",
    "\n",
    "Mathemetically, SFA is very simple. It's not successful, probably due to having a too strong prior, and it ignores slow moving objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 13.4 Sparse Coding\n",
    "\n",
    "### pp. 497 original sparse coding problem is intractable.\n",
    "\n",
    "> Training sparse coding with maximum likelihood is intractable. Instead, the training alternates between encoding the data and training the decoder to better reconstruct the data given the encoding.\n",
    "\n",
    "It's intractable in the sense that, if we marginalize out $h$, then it's intractable. In practice, we just have a point estimate of $h$, by solving lasso, assuming the posterior probability is very peaked, so a point estimate is accurate enough.\n",
    "\n",
    "> This approach will be justified further as a principled approximation to maximum likelihood later, in section 19.3.\n",
    "\n",
    "We should check this out later.\n",
    "\n",
    "### pp. 497 sparse coding model doesn't really give you many zero coefficients during generation.\n",
    "\n",
    "> If we sample $h$ from a Laplace prior, it is in fact a zero probability event for an element of $h$ to actually be zero. The generative model itself is not especially sparse, only the feature extractor is.\n",
    "\n",
    "This is exactly my experience. This is probably due to approximation in solving the sparse coding problem.\n",
    "\n",
    "### pp. 498 sparse coding has no generalization error for its encoder.\n",
    "\n",
    "\"no generalization error\" refers to that the encoder is optimal (given decoder weights, which can have generalization error).\n",
    "\n",
    "### pp. 498 nonparameteric nature of sparse coding makes it impossible to do backprop.\n",
    "\n",
    "> Another disadvantage is that it is not straight-forward to back-propagate through the non-parametric encoder, which makes it difficult to pretrain a sparse coding model with an unsupervised criterion and then fine-tune it using a supervised criterion. Modified versions of sparse coding that permit approximate derivatives do exist but are not widely used (Bagnell and Bradley, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Manifold Interpretation of PCA\n",
    "\n",
    "Here it talks about solution of linear autoencoder corresponds to PCA. See my notes on Autoencoder (Chapter 14), for a proof that, for a autoencoder with K hidden units, it's reconstruction error is the same as a K-component PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
