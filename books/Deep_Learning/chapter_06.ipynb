{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Example: Learning XOR\n",
    "\n",
    "### pp. 177 don't expect super clean result from gradient descent.\n",
    "\n",
    "> In practice, gradient descent would usually not find clean, easily understood, integer-valued solutions like the one we presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Gradient-Based Learning\n",
    "\n",
    "one central topic mentioned by authors over and over again is that for learning to work, there should be enough gradient. Thus, sigmoid units are not good, since they tend to saturate easily.\n",
    "\n",
    "\n",
    "### pp. 179 boundary probabilities can't be represented by many models.\n",
    "\n",
    "> For discrete output variables, most models are parametrized in such a way that they cannot represent a probability of zero or one, but can come arbitrarily close to doing so.\n",
    "\n",
    "this is because (most, I guess) exponential family can't describe boundary. See Theorem 3.3 of [Graphical Models, Exponential Families, and Variational Inference](http://dx.doi.org/10.1561/2200000001), as well as my special topic note on exponential family.\n",
    "\n",
    "Therefore, if the training data is perfectly separable, then some regularization is needed.\n",
    "\n",
    "### pp. 181 you need to match output units' type with cost function.\n",
    "\n",
    "> Unfortunately, mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that saturate produce very small gradients when combined with these cost functions.\n",
    "This is one reason that the cross-entropy cost function is more popular than mean squared error or mean absolute error, even when it is not necessary to estimate an entire distribution $p(y \\mid x)$.\n",
    "\n",
    "I think this is also connected to canonical link function in generalized linear models.\n",
    "\n",
    "### pp. 184 for sigmoid, mean squared error loss gives too little gradient, compared to maximum likelihood.\n",
    "\n",
    "> When we use other loss functions, such as mean squared error, the loss can saturate anytime $\\sigma(z)$ saturates. ... The gradient can shrink too small to be useful for learning whenever this happens, whether the model has the correct answer or the incorrect answer. For this reason, maximum likelihood is almost always the preferred approach to training sigmoid output units.\n",
    "\n",
    "Again, I think this is also connected to canonical link function in generalized linear models.\n",
    "\n",
    "### pp. 185 some intuition about softmax, about Eq. (6.30)\n",
    "\n",
    "> To gain some intuition for the second term, $\\log_j \\exp( z_j )$, observe that this term can be roughly approximated by $\\max_j z_j$. This approximation is based on the idea that $\\exp(z_k)$ is insignificant for any $z_k$ that is noticeably less than $\\max_j z_j$. ... This example will then contribute little to the overall training cost, which will be dominated by other examples that are not yet correctly classified.\n",
    "\n",
    "### pp. 186 again, why using correct cost function is correct.\n",
    "\n",
    "> When the softmax saturates, many cost functions based on the softmax also saturate, unless they are able to invert the saturating activating function.\n",
    "\n",
    "this inversion process is exactly what's done in (standard) generalized linear models.\n",
    "\n",
    "### pp. 187 overparameterized softmax, neuroscience connection, and naming of softmax\n",
    "\n",
    "> In practice, there is rarely much difference between using the overparametrized version or the restricted version, and it is simpler to implement the overparametrized version.\n",
    "\n",
    "> From a neuroscientific point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it.\n",
    "\n",
    "> The name “softmax” can be somewhat confusing. The function is more closely related to the arg max function than the max function. ... It would perhaps be better to call the softmax function “softargmax,” but the current name is an entrenched convention.\n",
    "\n",
    "### pp. 188-190 learning Gaussian output units and other extensions, such as GMM\n",
    "\n",
    "essentially, making sure positive definitiveness is important when learning covariance, and numerical stability is also important. Therefore,\n",
    "\n",
    "1. precision matrix is preferred, as it doesn't involve division when computing gradient.\n",
    "2. some parameterization tricks are needed to make sure positive definitiveness.\n",
    "\n",
    "In pp. 189, they talk about learning full covariance matrix, but their method can also guarantee semi-definitiveness. I think in practice, some small diagonal matrix can be always added. Same goes for learning GMM.\n",
    "\n",
    "In pp. 190, it says GMM optimization can be unreliable due to division. But I think this can be definitely avoided by using precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
