{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Learning algorithms\n",
    "\n",
    "### pp. 100 general strategy to deal with missing variables.\n",
    "\n",
    "> One way to efficiently define such a large set of functions is to learn a probability distribution over all of the relevant variables, then solve the classification task by marginalizing out the missing variables.\n",
    "\n",
    "### pp. 103 Density estimation is not the panacea for all ML tasks.\n",
    "\n",
    "> In principle, we can then perform computations on that distribution in order to solve the other tasks as well. ... In practice, density estimation does not always allow us to solve all of these related tasks, because in many cases the required operations on $p(x)$ are computationally intractable.\n",
    "\n",
    "### pp. 104 performance measure can be difficult to define or compute.\n",
    "\n",
    "> In some cases, this is because it is difficult to decide what should be measured. ... In other cases, we know what quantity we would ideally like to measure, but measuring it is impractical. ... Many of the best probabilistic models represent probability distributions only implicitly. Computing the actual probability value assigned to a specific point in space in many such models is intractable. In these cases, one must design an alternative criterion that still corresponds to the design objectives, or design a good approximation to the desired criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Capcity, Overfitting and Underfitting\n",
    "\n",
    "### pp. 111 two central challenges of ML\n",
    "\n",
    "> 1. Make the training error small.\n",
    "> 2. Make the gap between training and test error small.\n",
    ">\n",
    "> These two factors correspond to the two central challenges in machine learning: underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.\n",
    "\n",
    "### pp. 113-114 capacity of model also depends on learning algorithm.\n",
    "\n",
    "> the learning algorithm's effective capacity may be less than the representational capacity of the model family.\n",
    "\n",
    "### pp. 114 VC dimension related bounds are beautiful yet useless\n",
    "\n",
    "> These bounds provide intellectual justification that machine learning algorithms can work, but they are rarely used in practice when working with deep learning algorithms.\n",
    "\n",
    "### pp. 116 both model capacity and dataset size have to be correct.\n",
    "\n",
    "> Note that it is possible for the model to have optimal capacity and yet still have a large gap between training and generalization error. In this situation, we may be able to reduce this gap by gathering more training examples.\n",
    "\n",
    "### pp. 118 no universally best ML algorithm.\n",
    "\n",
    "> This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the “real world” that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.\n",
    "\n",
    "this means that when designing ML algorithms, it should be biased towards the real world. One type of bias is regularization.\n",
    "\n",
    "> The no free lunch theorem implies that we must design our machine learning algorithms to perform well on a specific task. We do so by building a set of preferences into the learning algorithm. When these preferences are aligned with the learning problems we ask the algorithm to solve, it performs better.\n",
    "\n",
    "### pp. 120 hypothesis space as regularization, and definition of regularization.\n",
    "\n",
    "> We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.\n",
    ">\n",
    "> Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Hyperparameters and Validation Sets\n",
    "\n",
    "### pp. 120-121 some parameters are not learnable from training data.\n",
    "\n",
    "> More frequently, the setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set.\n",
    "\n",
    "### pp. 121 ambiguous naming of validation set and training set.\n",
    "\n",
    "> The subset of data used to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process.\n",
    "\n",
    "### pp. 121 new datasets have to be made forever.\n",
    "\n",
    "> if we consider all the attempts from the scientific community at beating the reported state-of-the-art performance on that test set, we end up having optimistic evaluations with the test set as well. Benchmarks can thus become stale and then do not reflect the true field performance of a trained system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Estimators, Bias and Variance\n",
    "\n",
    "### pp. 130 two types of bias-variance decomposition\n",
    "\n",
    "In PRML, as well as elements of statistical learning, bias-variance decomposition is defined on performance measure (usually MSE). Here, it's defined on the estimated parameter (this is also the case for MLAPP). But in any case, these two are closely connected, by comparing Figure 5.3 and Figure 5.6 in this book.\n",
    "\n",
    "Actually I think bias-variance decomposition only applies to MSE, whether it's measuring performance or estimated parameter.\n",
    "\n",
    "> The relationship between bias and variance is tightly linked to the machine learning concepts of capacity, underfitting and overfitting. In the case where generalization error is measured by the MSE (where bias and variance are meaningful components of generalization error), increasing capacity tends to increase variance and decrease bias. This is illustrated in figure 5.6, where we see again the U-shaped curve of generalization error as a function of capacity.\n",
    "\n",
    "### pp. 131 asymptotic unbiasedness does not imply consistency.\n",
    "\n",
    "Check the given example.\n",
    "\n",
    "> However, the reverse is not true-asymptoticunbiasedness does not imply consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Maximum Likelihood Estimation\n",
    "\n",
    "### pp. 132 more general definition of \"cross-entropy\"\n",
    "\n",
    "> Many authors use the term “cross-entropy” to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
    "\n",
    "### pp. 132 we have no access to true distribution of data, and use training data as substitute.\n",
    "\n",
    "> Ideally, we would like to match the true data generating distributionpdata, but we have no direct access to this distribution.\n",
    "\n",
    "### pp. 133\n",
    "\n",
    "I feel that for conditional probability, such as Eq. (5.62), it's not appropriate to talk about KL or cross-entropy.\n",
    "\n",
    "### pp. 135 superiority of ML estimator\n",
    "\n",
    "> the Cramér-Rao lower bound (Rao, 1945; Cramér, 1946) shows that no consistent estimator has a lower mean squared error than the maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Bayesian Statistics\n",
    "\n",
    "### pp. 136 Bayesian is easier to justify.\n",
    "\n",
    "> This integral is of course just an application of the laws of probability, making the Bayesian approach simple to justify, while the frequentist machinery for constructing an estimator is based on the rather ad hoc decision to summarize all knowledge contained in the dataset with a single point estimate.\n",
    "\n",
    "### pp. 137 priors are problematic.\n",
    "\n",
    "> In practice, the prior often expresses a preference for models that are simpler or more smooth. Critics of the Bayesian approach identify the prior as a source of subjective human judgment impacting the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Supervised Learning Algorithms\n",
    "\n",
    "### pp. 144 k-NN methods are not discriminative. \n",
    "\n",
    "> The nearest neighbor of most points $x$ will be determined by the large number of features $x_2$ through $x_100$, not by the lone feature $x_1$ . Thus the output on small training sets will essentially be random.\n",
    "\n",
    "but I think this is also the problem for many other approaches. maybe this is the problem of generative/unsupervised approach, allocating too much power on useless stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Unsupervised Learning Algorithms\n",
    "\n",
    "### pp. 148 PCA is just first step to independence. It's only linear.\n",
    "\n",
    "> To achieve full independence, a representation learning algorithm must also remove the nonlinear relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Challenges Motivating Deep Learning\n",
    "\n",
    "### pp. 158 Only assuming smoothness is not enough to deal with curse of dimensionality.\n",
    "\n",
    "> Much of the modern motivation for deep learning is derived from studying the limitations of local template matching and how deep models are able to succeed in cases where local template matching fails.\n",
    "\n",
    "### pp. 160 additional assumptions are needed.\n",
    "\n",
    "> The key insight is that a very large number of regions, e.g., $O(2^k)$, can be defined with $O(k)$ examples, so long as we introduce some dependencies between the regions via additional assumptions about the underlying data generating distribution. In this way, we can actually generalize non-locally (Bengio and Monperrus, 2005; Bengio et al., 2006c). Many different deep learning algorithms provide implicit or explicit assumptions that are reasonable for a broad range of AI tasks in order to capture these advantages.\n",
    ">\n",
    "> Many other similarly generic assumptions can further improve deep learning algorithms. These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and the number of regions that can be distinguished. These exponential gains are described more precisely in sections 6.4.1, 15.4 and 15.5. The exponential advantages conferred by the use of deep, distributed representations counter the exponential challenges posed by the curse of dimensionality.\n",
    "\n",
    "Recurrent structure, or convolution, are all implicit assumptions. Using only k-NN is doomed to fail for high dimensional data.\n",
    "\n",
    "### pp. 161 an informal yet widely used definition of manifold.\n",
    "\n",
    "> Although there is a formal mathematical meaning to the term “manifold,” in machine learning it tends to be used more loosely to designate a connected set of points that can be approximated well by considering only a small number of degrees of freedom, or dimensions, embedded in a higher-dimensional space.\n",
    "\n",
    "### pp. 162 key assumption in high dimensional learning problems.\n",
    "\n",
    "> Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of $\\mathbb{R}^n$. Manifold learning algorithms surmount this obstacle by assuming that most of $\\mathbb{R}^n$ consists of invalid inputs ... the key assumption remains that probability mass is highly concentrated.\n",
    "\n",
    "Then, the author presents some evidence in favor of that high-dimensional real world data can be represented as a manifold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
