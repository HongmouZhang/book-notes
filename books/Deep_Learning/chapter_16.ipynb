{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 16.1 The Challenge of Unstructured Modeling\n",
    "\n",
    "Some sentences are confusing. In pp. 560, when talking about denoising requiring understanding the whole input\n",
    "\n",
    "> This requires multiple outputs (every element of the estimated clean example x) and an understanding of the entire input (since even one damaged area will still reveal the final estimate as being damaged).\n",
    "\n",
    "I don't understand \"one damaged area will still reveal the final estimate as being damaged\". However, it's sure that denoising requires understaning whole input. For example, the model should be able to handle noise at every location, so no entry in $x$ can be ignored.\n",
    "\n",
    "Well, I think the line between understanding input and not understanding is blurry. For example, what if I use a CNN to predict each pixel's clean version, based on a large surrounding around it? $p(x)$ itself may not be needed when doing denoising.\n",
    "\n",
    "### pp. 562 why table-based distribution modeling is unnecessary.\n",
    "\n",
    "The problem with the table-based approach is that we are explicitly modeling every possible kind of interaction between every possible subset of variables. The probability distributions we encounter in real tasks are much simpler than this. Usually, most variables influence each other only indirectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 16.2 Using Graphs to Describe Model Structure\n",
    "\n",
    "### pp. 568 partion function may not exist.\n",
    "\n",
    "> One important consideration to keep in mind when designing undirected models is that it is possible to specify the factors in such a way that Z does not exist.\n",
    "\n",
    "I think the utility of such improper distribution depends on specific use cases. For example, an improper flat prior when computing MAP estimate is ok, as long as there's data.\n",
    "\n",
    "### pp. 569 domain of variable can change undirected models drastically.\n",
    "\n",
    "> One key idea to keep in mind while working with undirected models is that the domain of each of the variables has dramatic effect on the kind of probability distribution that a given set of $\\phi$ functions corresponds to. ... Often, it is possible to leverage the effect of a carefully chosen domain of a variable in order to obtain complicated behavior from a relatively simple set of $\\phi$ functions. We will explore a practical application of this idea later, in section 20.6.\n",
    "\n",
    "### pp. 572\n",
    "\n",
    "Eq. (16.8) gives definition of free energy. May encounter it in some literature.\n",
    "\n",
    "### pp. 573 some independencies are not representable in PGM's language.\n",
    "\n",
    "> In fact, some distributions contain independences that are not possible to represent with existing graphical notation. Contextspecific independences are independences that are present dependent on thevalue of some variables in the network.\n",
    "\n",
    "### pp. 576 probability distributions are not intrinsically directed or undirected.\n",
    "\n",
    "> We often refer to a specific machine learning model as being undirected or directed. For example, we typically refer to RBMs as undirected and sparse coding as directed. This choice of wording can be somewhat misleading, because no probabilistic model is inherently directed or undirected. Instead, some models are most easily described using a directed graph, or most easily described using an undirected graph.\n",
    "\n",
    "### pp. 579 factor graphs don't give new conditional independence statements.\n",
    "\n",
    "I don't have a proof of this. But Jordan's book and Koller's book all say so. Check my special notes on MRF (`graphical_models/02_MRF/02_Markov_Random_Fields.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5 Learning about Dependencies\n",
    "\n",
    "Here two approaches are introduced.\n",
    "\n",
    "1. when there's no hidden, learn local dependencies between visible units. I think examples include graph lasso.\n",
    "2. model visible using hidden. this is the approach taken by RBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7 The Deep Learning Approach to Structured Probalistic Models.\n",
    "\n",
    "### pp. 585 traditional PGM vs Deep Learning PGM\n",
    "\n",
    "Overall number of hidden units and visible units\n",
    "\n",
    "> Deep learning essentially always makes use of the idea of distributed representations. ... Deep learning models typically have more latent variables than observed variables. Complicated nonlinear interactions between variables are accomplished via indirect connections that flow through multiple latent variables.\n",
    "\n",
    "> By contrast, traditional graphical models usually contain mostly variables that are at least occasionally observed, even if many of the variables are missing at random from some training examples. Traditional models mostly use higher-order terms and structure learning to capture complicated nonlinear interactions between variables. If there are latent variables, they are usually few in number.\n",
    "\n",
    "Semantics of hidden units.\n",
    "\n",
    "> The deep learning practitioner typically does not intend for the latent variables to take on any specific semantics ahead of time—the training algorithm is free to invent the concepts it needs to model a particular dataset.\n",
    "\n",
    "> When latent variables are used in the context of traditional graphical models, they are often designed with some specific semantics in mind—the topic of a document, the intelligence of a student, the disease causing a patient’s symptoms, etc. These models are often much more interpretable by human practitioners and often have more theoretical guarantees, yet are less able to scale to complex problems and are not reusable in as many different contexts as deep models.\n",
    "\n",
    "Connectivity patterns\n",
    "\n",
    "> Deep graphical models typically have large groups of unitsthat are all connected to other groups of units, so that the interactions betweentwo groups may be described by a single matrix.\n",
    "\n",
    "> Traditional graphical models have very few connections and the choice of connections for each variable may be individually designed. The design of the model structure is tightly linked with the choice of inference algorithm.\n",
    "\n",
    "Inference\n",
    "\n",
    "> Traditional approaches to graphical models typically aim to maintain the tractability of exact inference.\n",
    "\n",
    "> By comparison, models used in deep learning tend toconnect each visible unit $v_i$ to very many hidden units $h_j$, so that h can provide a distributed representation of $v_i$ (and probably several other observed variables too). Distributed representations have many advantages, but from the point of view of graphical models and computational complexity, distributed representations have the disadvantage of usually yielding graphs that are not sparse enough for the traditional techniques of exact inference and loopy belief propagation to be relevant.\n",
    "\n",
    "I think \"relevant\" means accurate enough for loopy belief propagation.\n",
    "\n",
    "> As a consequence, one of the most striking differences between the larger graphical models community and the deep graphical models community is that loopy belief propagation is almost never used for deep learning.\n",
    "\n",
    "Finally, there is a lot of approximation and guesswork in Deep Learning\n",
    "\n",
    "> Finally, the deep learning approach to graphical modeling is characterized by a marked tolerance of the unknown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
