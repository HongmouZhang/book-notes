{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes for Chapter 1 of *Theoretical Neuroscience* by P. Dayan and L. F. Abbott."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Spike Trains and Firing Rates\n",
    "\n",
    "### pp. 10, below Eq. (1.5) spiking probability from firing rate\n",
    "\n",
    ">  If $\\Delta t$ is small, there will never be more than one spike within the interval between $t$ and $t + \\Delta t$ on any given trial. This means that $\\mathbf{r}(t) \\Delta t$ is also the fraction of trials on which a spike occurred between those times. Equivalently, $\\mathbf{r}(t) \\Delta t$ is the probability that a spike occurs during this time interval.\n",
    "\n",
    "### pp. 10, below Eq. (1.6) interchangeability of trial average spike train and firing rate.\n",
    "\n",
    "Comments on Eq. (1.6): I think trial average spike train is just some ideal stuff. You can never obtain it in real data. The writing here suggests that, we first have spike train, and then have firing rate as an average in the limit. I think it would be easier to think the other way round, which is how modeling is done in practice.\n",
    "\n",
    "* The spike train is a sampling of the firing rate function\n",
    "* we can use a large number of spike trains to recover the firing rate function.\n",
    "\n",
    "### pp. 11, below Eq. (1.7) three types of firing rates.\n",
    "\n",
    "firing rate, spike-count rate, and average firing rate.\n",
    "\n",
    "### pp. 11, Measuring firing rates.\n",
    "\n",
    "There are a lot of options in measuring firing rate from finite data.\n",
    "\n",
    "### pp. 14, Eq. (1.12) causal kernel window\n",
    "\n",
    "this may or may not make sense. It depends.\n",
    "\n",
    "### pp. 14 Tuning Curves\n",
    "\n",
    "different tasks and neurons may be best described by different tuning curves.\n",
    "\n",
    "* for motor neurons, people usually use cosine tuning curve. Notice that this would make it impossible to control the width of the tuning curve. However, Xiao Zhou told me that this is true for most of motor neurons (that they have a tuning with that is the same as that of cosine).\n",
    "    * However, I think it's possible to introduce some width factor, changing $\\cos(s-s_{max})$ to $\\cos(w(s-s_{max}))$ \n",
    "    * <http://www.gatsby.ucl.ac.uk/~aguez/tn1/slides/popcoding-slides.pdf> has some more tuning curve models, such as wrapped Gaussian (not sure how to fit it; [wiki](https://en.wikipedia.org/wiki/Wrapped_normal_distribution) says it can be approximated well by von Mises).\n",
    "    ![](./chapter_01/tuning_curve.png)\n",
    "\n",
    "### pp. 16-17 spike-count variability.\n",
    "\n",
    "two types of noise models are often used.\n",
    "\n",
    "1. additive noise. noise distribution is independent of mean firing rate.\n",
    "2. multiplicative noise. std of noise distribution is proportional to mean firing rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 What Makes a Neuron fire?\n",
    "\n",
    "### pp. 18 Describing the stimulus.\n",
    "\n",
    "* Here they mentioned Weber's law and Fechner's law. It's used in many other works, such as [Constrained sampling experiments reveal principles of detection in natural scenes](http://dx.doi.org/10.1073/pnas.1619487114). Notice that in that paper. In that paper, they cited Weber's law, in the form that change of stimulus is linear to stimulus amplitude.\n",
    "    * one potential problem is, should we use perceived stimulus amplitude or actual. I think using the latter makes less assumption.\n",
    "* They assume that average stimulus amplitude is zero. I think in practice we just assume that neurons have adaptation and use the mean amplitude of stimulus as zero.\n",
    "\n",
    "### pp. 19 periodic stimulus.\n",
    "\n",
    "This is just a convenience for math analysis. Should not matter much in practice, if stimulus is long.\n",
    "\n",
    "### pp. 19 STA \n",
    "\n",
    "Eq. (1.19) has some approximations. In practice, I think people just concatenate all trials into one big one, and we have no approximation. However, from a theoretic point of view as to connect STA to correlation as in Eq. (1.22), we should have multiple trials. The main reason we want to do this approximation is that we can use Eq. (1.2) and exchange of average sign with integral on right hand side of second equal sign for Eq. (1.19), and then use Eq. Eq. (1.6) to get right hand side of second equal sign for Eq. (1.20).\n",
    "\n",
    "### pp. 20 Eq. (1.22) why STA is called reverse correlation.\n",
    "\n",
    "### pp. 22 white noise stimuli.\n",
    "\n",
    "1. we can't have ideal white noise stimuli that follow Eq. (1.24). We can only have stimuli that follow Eq. (1.25).\n",
    "2. White and Gaussian are different. Gaussian is white, and white can be non-Gaussian. However, according to [Liam's paper Convergence properties of three spike-triggered analysis techniques](http://dx.doi.org/10.1088/0954-898X_14_3_304) or [here](http://www.stat.columbia.edu/~liam/research/pubs/sta-network.pdf), we'd better use Gaussian for theoretic gurantees. However, in practice, maybe other stimuli make excite neurons better. See [Spatiotemporal elements of macaque V1 receptive fields](https://doi.org/10.1016/j.neuron.2005.05.021)\n",
    "\n",
    "### pp. 23 other spike triggered statistics.\n",
    "\n",
    "* Fig. (1.10) shows that multiple-spike triggered statistics can not be decomposed into those of single-spike ones. Not sure how this holds or does not hold when using different stimuli.\n",
    "* Spike-triggered average autocorrelation. Not sure what it is. I think it's some variation of Eq. (1.23), where a stimulus only counts when there's a spike, etc. In this sense, spike train acts as a weighting term."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
