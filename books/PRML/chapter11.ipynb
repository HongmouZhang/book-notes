{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a collection of highlights and comments for Chapter 11 of PRML book. page numbers are probably based on first printing of the book. (I only have a PDF version of the book's (possibly) first printing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.0\n",
    "\n",
    "#### pp. 523\n",
    "\n",
    "> Although for some applications the posterior distribution over unobserved variables will be of direct interest in itself, for most situations the posterior distribution is required primarily for the purpose of evaluating expectations, for example in order to make predictions. The fundamental problem that we therefore wish to address in this chapter involves finding the expectation of some function $f(z)$ with respect to a probability distribution $p(z)$.\n",
    "\n",
    "#### pp. 524\n",
    "\n",
    "> we note that if $f(z)$ is small in regions where $p(z)$ is large, and vice versa, then the expectation may be dominated by regions of small probability, implying that relatively large sample sizes will be required to achieve sufficient accuracy.\n",
    "\n",
    "#### pp. 525 ancestral sampling with observed variables will not work in general.\n",
    "\n",
    "> However, the overall probability of accepting a sample from the posterior decreases rapidly as the number of observed variables increases and as the number of states that those variables can take increases, and so this approach is rarely used in practice.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.1 Basic Sampling Algorithms\n",
    "\n",
    "### 11.1.1 Standard distributions\n",
    "\n",
    "Essentially, we are using change of variable for random variable. We sample from some easy to sample distribution, and change variable. According to change of variable rule, the transformed one will follow another distribution. There are two examples. the one around Eq. (11.5)-(11.7) is where the easy to sample distribution is uniform distribution on (0,1), and the transform is simply inverse of CDF of the desired distribution. Another example is 2D Gaussian, from Eq. (11.10)-Eq. (11.12).\n",
    "\n",
    "#### pp. 528 Such methods won't apply in general, (considering 1D case) because it requires computing CDF and invert it, which is not feasible in general.\n",
    "\n",
    "> Obviously, the transformation technique depends for its success on the ability to calculate and then invert the indefinite integral of the required distribution. Such operations will only be feasible for a limited number of simple distributions, and so we must turn to alternative approaches in search of a more general strategy.\n",
    "\n",
    "### 11.1.2 Rejection Sampling\n",
    "\n",
    "#### pp. 528 to perform rejection sampling, we just need to know to compute unnormalized PDF.\n",
    "\n",
    "> Furthermore suppose, as is often the case, that we are easily able to evaluate $p(z)$ for any given value of $z$, up to some normalizing constant $Z$\n",
    "\n",
    "#### pp. 529\n",
    "\n",
    "> This pair of random numbers has uniform distribution under the curve of the function $kq(z)$.\n",
    "\n",
    "here it's \"uniform\" in terms of area. area of any subregion under comparison function over all area is probability that a point from that area will be sampled.\n",
    "\n",
    "### 11.1.3 Adaptive rejection sampling\n",
    "\n",
    "This trick only works for log concave functions (so the log probability of PDF is concave). Thus we can find lines in log space or exp of lines in original space as upper bound for PDF. This allows tighter upper bound than the naive rejection sampling.\n",
    "\n",
    "### pp. 531-532 rejection sampling doesn't work in high D.\n",
    "\n",
    "\n",
    "> The acceptance rate will be the ratio of volumes under $p(z)$ and $kq(z)$, which, because both distributions are normalized, is just $1/k$. Thus the acceptance rate diminishes exponentially with dimensionality.\n",
    "\n",
    "> Furthermore, the exponential decrease of acceptance rate with dimensionality is a generic feature of rejection sampling. Although rejection can be a useful technique in one or two dimensions it is unsuited to problems of high dimensionality. It can, however, play a role as a subroutine in more sophisticated algorithms for sampling in high dimensional spaces.\n",
    "\n",
    "### 11.1.4 Importance Sampling\n",
    "\n",
    "#### pp. 532 intuition of importance sampling\n",
    "\n",
    "> We would really like to choose the sample points to fall in regions where $p(z)$ is large, or ideally where the product $p(z)f(z)$ is large.\n",
    "\n",
    "#### pp. 533 biased importance sampling (Eq. (11.20) - (11.23))\n",
    "\n",
    "DL book says this method is biased. Indeed it’s not easy to show it’s unbiased. I think the product of two unbiased yet correlated stuff probably would yield biased results.\n",
    "\n",
    "#### pp. 534 importance sampling requires proposal distribution to be very well chosen.\n",
    "\n",
    "> This also highlights a key requirement for the sampling distribution $q(z)$, namely that it should not be small or zero in regions where $p(z)$ may be significant.\n",
    "\n",
    "#### pp. 534 additional extensions to importance sampling \n",
    "\n",
    "for the uniform sampling, the notatino is sloppy. essentially all $p(z), \\tilde{p}(z)$ shjould be $p(z,x), \\tilde{p}(z,x)$, if we use $z$ to denote all variables unobserved (thus need sampling), p(z|x) = p(z,x) up to a multiplicative constant. done.\n",
    "\n",
    "I ignored likelihood weighted sampling, which is probably some better way for $q(z)$ (proposal distribution).\n",
    "\n",
    "### 11.1.5 Sampling-importance-resampling\n",
    "\n",
    "Essentially, this approach gives us a better proposal distribution, using weights obtained in biased importance sampling. The derivation Eqs. (11.25) and (11.26) is talking about one sample from the second set of L samples.\n",
    "\n",
    "last line of pp. 535, \"if moments\" should be \"if functions\".\n",
    "\n",
    "### 11.1.6 Sampling and the EM algorithm\n",
    "\n",
    "#### pp. 537 IP algorithm\n",
    "\n",
    "the presentation of this algorithm is not good. In Eq. (11.30) and Eq. (11.31), actually the integrand is both $p(Z,\\theta \\mid X)$. In the I step, we get many $Z, \\theta$ pairs (well here actually we need a sample of $Z$); in the P step, we ignore all sampled $\\theta$, and then replace $p(Z\\mid X)$ in Eq. (11.31) with our sampled $Z$. That's all. Essentially, replacing some integration by sampling.\n",
    "\n",
    "Notice that from the perspective of sampling, $Z, \\theta$ are the same. That's probably why the author says \"From now on, we blur this distinction and focus simply on the problem of drawing samples from a given posterior distribution\", at end of 11.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.2 Markov Chain Monte Carlo\n",
    "\n",
    "Some general comments.\n",
    "\n",
    "1. MCMC methods are defined by the transition probabilities.\n",
    "2. To make MCMC algorithm work, we need\n",
    "    1. the desired distribution $p(x)$ is a (may not be unique) invariant distribution for transition. A sufficient condition for this is detailed balance, seen in Eq. ((11.40).\n",
    "    2. ergodicity. that means regardless of initial distribution of $x$, after infinite number of transitions, the distribution will converge to $p(x)$. This time, the (unique) invariant distribution is also called equilibrium distribution.\n",
    "3. in this book, when proving the correctness of a MCMC algorithm, usually only point 1 is proved, either directly or through detailed balance; and ergodicity is left unproved. But let's just assume it's correct.\n",
    "\n",
    "\n",
    "### pp. 539 general problem of MCMC methods\n",
    "\n",
    "naive MCMC methods (Gibbs, also MH) are often subject to random walk behavior. This is also discussed in DL book (17.5).\n",
    "\n",
    "> This square root dependence is typical of random walk behaviour and shows that random walks are very inefficient in exploring the state space. As we shall see, a central goal in designing Markov chain Monte Carlo methods is to avoid random walk behaviour.\n",
    "\n",
    "\n",
    "\n",
    "### 11.2.1 Markov chains\n",
    "\n",
    "#### pp. 541 composition of transitions.\n",
    "\n",
    "You can compose transitions using sum (Eq. (11.42)) or product (Eq. (11.43)). For product, there's some requirement for it to satisfy detailed balance.\n",
    "\n",
    "> This does not hold for the transition probability constructed using (11.43), although by symmetrizing the order of application of the base transitions, in the form B1,B2,...,BK,BK,...,B2,B1, detailed balance can be restored. A common example of the use of composite transition probabilities is where each base transition changes only a subset of the variables.\n",
    "\n",
    "However, I don't see in this book, that detailed balance is proved through this symmetry. Usually, it's directly through some property of transitions themselves. Such as in the case of Gibbs sampling.\n",
    "\n",
    "### 11.2.2 The Metropolis-Hastings algorithm\n",
    "\n",
    "#### pp. 542 MH can exhibit random walk behavior, mixing very slowly.\n",
    "\n",
    ">  These details aside, it remains the case that if the length scales over which the distributions vary are very different in different directions, then the Metropolis Hastings algorithm can have very slow convergence.\n",
    "\n",
    "> In fact in two dimensions, the increase in rejection rate as $\\rho$ increases is offset by the larger steps sizes of those transitions that are accepted\n",
    "\n",
    "no matter your step size, convergence is going to be slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.3 Gibbs Sampling\n",
    "\n",
    "#### pp. 542-543 order of sampling is arbitrary\n",
    "\n",
    "> This procedure is repeated either by cycling through the variables in some particular order or by choosing the variable to be updated at each step at random from some distribution.\n",
    "\n",
    "#### pp. 544 proof of Gibbs sampling\n",
    "\n",
    "that $p(z)$ is invariant distribution can be proved step by step.\n",
    "\n",
    "> note that the distribution $p(z)$ is an invariant of each of the Gibbs sampling steps individually and hence of the whole Markov chain.\n",
    "\n",
    "#### pp. 544 Gbbis sampling is MH\n",
    "\n",
    "Check Eq. (11.49)\n",
    "\n",
    "> We can obtain the Gibbs sampling procedure as a particular instance of the Metropolis-Hastings algorithm as follows.  ... Thus the Metropolis-Hastings steps are always accepted.\n",
    "\n",
    "#### pp. 545 over relaxation to mitigate random walk behavior\n",
    "\n",
    "Check Eq. (11.50)\n",
    "\n",
    "> The effect of over-relaxation is to encourage directed motion through state space when the variables are highly correlated. The framework of ordered over-relaxation (Neal, 1999) generalizes this approach to non-Gaussian distributions.\n",
    "\n",
    "#### pp. 546 ICM as greedy Gibbs\n",
    "\n",
    "> If, at each stage of the Gibbs sampling algorithm, instead of drawing a sample from the corresponding conditional distribution, we make a point estimate of the variable given by the maximum of the conditional distribution, then we obtain the iterated conditional modes (ICM) algorithm discussed in Section 8.3.3. Thus ICM can be seen as a greedy approximation to Gibbs sampling.\n",
    "\n",
    "#### pp. 546 block Gibbs\n",
    "\n",
    "> Because the basic Gibbs sampling technique considers one variable at a time, there are strong dependencies between successive samples. At the opposite extreme, if we could draw samples directly from the joint distribution (an operation that we are supposing is intractable), then successive samples would be independent. We can hope to improve on the simple Gibbs sampler by adopting an intermediate strategy in which we sample successively from groups of variables rather than individual vari- ables. This is achieved in the blocking Gibbs sampling algorithm by choosing blocks of variables, not necessarily disjoint, and then sampling jointly from the variables in each block in turn, conditioned on the remaining variables (Jensen et al., 1995).\n",
    "\n",
    "Notice that blocks of variables are not necessarily disjoint. But I think in Deep Learning, the block Gibbs is always simply used as a speed up to naive Gibbs, because all variables in one block are independent given others, so the result is precisely the same as that of naive Gibbs. I think if we use general Gibbs, then it might be a little tricky to prove things like ergodicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11.4 Slice Sampling\n",
    "\n",
    "In Eq. (5.11), $1/Z_p$ is the correct normalization function, since area under curve $\\tilde{p}(z)$ is $Z_p$. Let's not care about why this is correct. Just think about the intuition of changing step size. \n",
    "\n",
    "#### pp. 547\n",
    "\n",
    ">  It is in the choice of this region that the adaptation to the characteristic length scales of the distribution takes place.\n",
    "\n",
    "I don't know how well this intuition can work in high D. How can you slice a high D stuff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 The Hybrid Monte Carlo Algorithm\n",
    "\n",
    "While here seems that HMC is really recommended by the author, the MLAPP book says that this method is sensitive to some parameters. Anyway.\n",
    "\n",
    "#### pp. 548 it can make big steps without high rejection!\n",
    "\n",
    "> The problem cannot be resolved simply by taking bigger steps as this leads to a high rejection rate. ... [Hamiltonian dynamical system] has the property of being able to make large changes to the system state while keeping the rejection probability small.\n",
    "\n",
    "#### pp. 548 this benefits comes with limitation that we must know gradient information.\n",
    "\n",
    "> It is applicable to distributions over continuous variables for which we can readily evaluate the gradient of the log probability with respect to the state variables.\n",
    "\n",
    "### 11.5.1 Dynamical systems\n",
    "\n",
    "#### pp. 550 two important properties of Hamiltonian dynamical systems\n",
    "\n",
    "First, shown in Eq. (11.60), is that $H(z,r)$ is constant relative to time; second, shown in Eq. (11.61), (11.62), is that the volume of a region in phase space stays the same under time. I don't know why Eq. (11.62) implies that volume stays the same; just assume this is correct; forgot much of calculus...\n",
    "\n",
    "Later on, the author says that we can use these two properties to show the distribution defined by $H$ is invariant over time. Actually I think simply using the fact that $H(z,r)$ is constant over time is fine. Why using some region ... But I would say using volume is close to the definition of probability density. However, I would say $\\exp(-H(z,r))$ already defines the density... anyway. However, this constant volume property is needed when proving detailed balance in Section 11.5.2.\n",
    "\n",
    "#### pp. 550 dynamics of Hamiltonian system can avoid random walk.\n",
    "\n",
    "> Although H is invariant, the values of z and r will vary, and so by integrating the Hamiltonian dynamics over a finite time duration it becomes possible to make large changes to z in a systematic way that avoids random walk behaviour.\n",
    "\n",
    "Not sure whether there's a proof on this.\n",
    "\n",
    "#### pp. 551 pp. fix for ergodicity.\n",
    "\n",
    "BTW, I don't have a proof why this alone could assure ergodicity. But it makes sense.\n",
    "\n",
    "> Evolution under the Hamiltonian dynamics will not, however, sample ergodially from $p(z, r)$ because the value of $H$ is constant. ... The simplest way to achieve this is to replace the value of $r$ with one drawn from its distribution conditioned on $z$.\n",
    "\n",
    "$p(r\\mid z)$ is simply $p(r)$, since they are independent in $H$. so easy to sample. Intuitively, $z$ part has no random walk behavior, and $r$ part is very easy to sample. So we are good.\n",
    "\n",
    "#### pp. 551 summary\n",
    "\n",
    "> In summary then, the Hamiltonian dynamical approach involves alternating between a series of leapfrog updates and a resampling of the momentum variables from their marginal distribution.\n",
    "\n",
    "#### pp. 551 why is it more powerful\n",
    "\n",
    "> Informally, this follows from the fact that in a space of dimension D, the additional computational cost of evaluating a gradient compared with evaluating the function itself will typically be a fixed factor independent of D, whereas the D-dimensional gradient vector conveys D pieces of information compared with the one piece of information given by the function itself.\n",
    "\n",
    "### 11.5.2 Hybrid Monto Carlo\n",
    "\n",
    "Essentially, it's a fix to the non ideal integration in leaffrog dynamics evolution.\n",
    "\n",
    "#### pp. 552 leapfrog preserves volume.\n",
    "\n",
    "> Next we show that the leapfrog integration preserves phase-space volume exactly. \n",
    "\n",
    "Yeah, although leapfrog may not be a little in exact, if we leap frog a region, actually volume is preserved. As in Figure 11.14, Consider we start from a rectangular region, and $z$ and $r$ are both 1-d. Then when update $r_i$, consider each column slice of points with same $z_i$ but different $r_i$. By leapfrog eq, you will know that the change of $r_i$ to all these points are the same (this same change, I believe, is what the author means by shearing), so the volume is preserved. Same applies when updating $z_i$. Thus, overall, we never change volume.\n",
    "\n",
    "#### pp. 552 how detailed balance holds\n",
    "\n",
    "I need an explanation on Eq. (11.68). Essentially, this is only an informal proof, with ergodicity missing.\n",
    "\n",
    "I guess here we are proving that, given any small volume, if we consider them as a discrete state, then detailed balance holds?\n",
    "\n",
    "I think first, the authors choose one very small region $R$, transitioned to $R'$ with $L$ leaps, such that we can consider any point $R$ to have same value of $H$. This is also true for $R'$. So $H(R)$ or $H(R')$ just mean the value of $H$ when taking some arbitrary value in those regions.\n",
    "\n",
    "Suppose that two regions are non-overlapping (which is also assumed by the author as mentioned under Eq. (11.69)). I think it also assumes that, by backwards L steps from $R$, the region $R''$ is non overlapping with $R$ and $R'$. Otherwise, if there's overlap, then there might be multiple ways to get from $R$ to $R'$ (backwards or forwards), and Eq. (11.68) won't hold.\n",
    "\n",
    "Mapping this back to Eq. (11.40) (detailed balance). The event that our point is inside $R$ is $z$, and the event that our point is inside $R'$ is $z'$. In theory, we should also discrete the other parts of the phase space, and then prove Eq. (11.40) holds when $z'$ is of any other region, not just $R'$. But this is trivial, because in other cases, $T(z,z')$ is simply (almost always) zero (since our T exactly takes L steps). Well it's non zero only for two states. One is the state of falling in $R'$ (which we already proved), and another one is the sate of falling in a region by backwards L leaps. But the proof for that one should be similar to the one shown in the book. So our proof is done for a specific choice of $z$. We can change $z$ to some other region, and do the proof again. Thus we proved Eq. (11.40), with some discretization, and mild assumptions.\n",
    "\n",
    "\n",
    "In Eq. (11.68), the part up to $\\delta V$ is probability of starting from some point in R. Later part is using rejection.\n",
    "\n",
    "Having 11.68 alone should not be sufficient to be ergodic. Problem is, if we only use H dynamics, then many terms in transition matrix is zero, and this may not make the chain ergodic.\n",
    "\n",
    "Thus, maybe this is why sampling r is needed. \n",
    "\n",
    "The 1/2 term, introduced by randomly choosing going backward or forward, is just to make sure detailed balance works, otherwise, this transition will not allow you to go from R' to R. However, this fix may not be incomplete, as still there are too many places not able to be transitioned to, thus this transition doesn't look like ergodic to me.\n",
    "\n",
    "\n",
    "1/2 is for choosing forward. Are you suggesting that we can’t reach R’ when doing backwards? I think here it assumes that our step size is small enough, such that there’s no way for getting to R’ in -L steps. Or, even it is, still the detailed balance still holds, except that 1/2 both vanish in 11.68 and  11.69\n",
    "\n",
    "### pp. 553 more work is needed to ensure ergodicity.\n",
    "\n",
    "> In such cases, the random replacement of the momentum values before each leapfrog integration will not be sufficient to ensure ergodicity because the position variables will never be updated. Such phenomena are easily avoided by choosing the magnitude of the step size at random from some small interval, before each leapfrog integration.\n",
    "\n",
    "### pp. 554 HMC is orders of magnitude faster.\n",
    "\n",
    ">  In order to achieve this, the leapfrog integration must be continued for a number of iterations of order $\\sigma_{max}/\\sigma_{min}$. ... By constrast, ... The exploration of state space then proceeds by a random walk and takes of order $(\\sigma_{max}/\\sigma_{min})^2$ steps to arrive at a roughly independent state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Estimating the Partition Function\n",
    "\n",
    "For Eq. (11.73), Notice that 11.73 is a normalized distribution for $z$. Because each $T(z^{(l)}, z)$  is normalized distribution for $z$, and average of $L$ of them must be as well. right side should have $1/L$.\n",
    "\n",
    "Essentially, the author says we can use a finite number of samples from any MCMC chain to define a G distribution (by averaging the transition conditional probabilities of these samples points). By using a good chain, maybe we can make G better. Here the specific procedure is not given (how to choose the MCMC chain to make sure that average of $T$'s is a good $G$)\n",
    "\n",
    "Notice that I think L in 11.73 and L in 11.72 are different. One used for defining G, another for computing ratio of $Z_E$ and $Z_G$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
