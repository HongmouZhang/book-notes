{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of highlights and comments for Chapter 2 of PRML book. page numbers are probably based on first printing of the book. (I only have a PDF version of the book's (possibly) first printing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Binary Variables\n",
    "### pp. 71 Beta distribution.\n",
    "\n",
    "PRML doesn't address what to do when $\\mu=0$ and $\\mu=1$. According to <http://www.math.uah.edu/stat/special/Beta.html>,\n",
    "\n",
    "> if $a \\geq 1$, $f$ is defined at 0, and if $b \\geq 1$, $f$ is defined at 1. In these cases, it's customary to extend the support set to these endpoints.\n",
    "\n",
    "But in any case, these border points are irrelevant. If they are, we can use left/right limit to obtain their values if needed (which is in general a good way to obtain the value of a function at undefined points)\n",
    "\n",
    "### pp. 74\n",
    "\n",
    "> uncertainty represented by the posterior distribution will steadily decrease.\n",
    "\n",
    "well, (2.21) holds because your prior is correct by design, as $p(D, \\theta) = p(D\\mid\\theta) p(\\theta)$.\n",
    "\n",
    "> that this result only holds on average,\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multinomial Variables\n",
    "\n",
    "### misc\n",
    "\n",
    "Why likelihood and prior have same form, but different names (binomial (or Bernoulli) vs beta), (multinomial vs. Dirichlet): when we say likelihood and prior have the same form, we mean they are of the same form w.r.t. the parameters. When we talk about those more common names for likelihood, (binomial, multinomial, Gaussian), we are talking about the distribution of the random variable patameterized by those parameters, and this random variable is different from the parameter themselves.\n",
    "\n",
    "While by [definition](https://en.wikipedia.org/wiki/Conjugate_prior), conjugate prior means that posterior having same form as prior, in the case of exponential family, actually prior, posterior, and likelihood, all have the same form. Check Eq. (230) on pp. 117 to see why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The Gaussian Distribution\n",
    "\n",
    "### pp. 92\n",
    "\n",
    "> (2.110) A special case of this result is when $A = I$, in which case it reduces to the convolution of two Gaussians\n",
    "\n",
    "To be more precise, $p_(y \\mid x)$ has a mean of $x + b$, and $p(x)$ has a mean of $\\mu$.  Here, the marginal $p(y)$ is the convolution of two distributions, one of mean $\\mu$, and one of mean b.\n",
    "\n",
    "Below is why this is correct.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(x) &= \\int p_1(x-x_2) p_2(x_2) \\mathrm{d} x_2 \\\\\n",
    "    & = \\int N(x - x_2 \\mid \\mu_1, \\tau_1^{-1}) N(x_2 \\mid \\mu_2, \\tau_2^{-1}) \\mathrm{d} x_2 \\\\\n",
    "    & = \\int N(x \\mid \\mu_1+x_2, \\tau_1^{-1}) N(x_2 \\mid \\mu_2, \\tau_2^{-1}) \\mathrm{d} x_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, $N(x \\mid \\mu_1+x_2, \\tau_1^{-1})$ is $p_(y \\mid x)$, and $N(x_2 \\mid \\mu_2, \\tau_2^{-1})$ is $p(x)$.\n",
    "\n",
    "### pp. 94\n",
    "\n",
    "Here the author talks about Robbins-Monro. Basically, it's formulating sequential estimation in the framework of [Stochastic approximation](https://en.wikipedia.org/wiki/Stochastic_approximation). Check this link for a probably clearer description about relationship between $z$ and $\\theta$. Basically, $z$ is a noisy version of $f$, which is a function of $\\theta$.\n",
    "\n",
    "One possibly unintuitive part about this Robbins-Monro is Eq. (2.129) (check errata and wiki, it should be minus sign). The basic problem is that, how can we estimate $\\theta$ by revising our current estimate by $z$? I mean, $f$ and $z$ and functions, and $\\theta$ is argument to function, how can you mix them together. This is because we assumed that $f$ is a monotonically increasing function in $\\theta$. Consider an extreme case, where $z$ is a noiseless, so Eq. (2.129) means that, if our estimate is big, (so $f(\\theta) > 0$), we subtract the current estimate by some positive number, and vice versa. So this procedure looks correct for such simple cases.\n",
    "\n",
    "### pp. 101\n",
    "\n",
    "> we shall see that the interpretation of a conjugate prior in terms of effective fictitious data points is a general one for the exponential family of distributions.\n",
    "\n",
    "Check Eq. (230) on pp. 117 to see why.\n",
    "\n",
    "### pp. 103\n",
    "\n",
    "> t-distribution is obtained by adding up an infinite number of Gaussian distributions having the same mean but different precisions. This can be interpreted as an infinite mixture of Gaussians.\n",
    "\n",
    "This is why t is more robust than Gaussian.\n",
    "\n",
    "### pp. 106\n",
    "\n",
    "I think Eq. (2.169) should be `atan2`, so is Eq. (2.184).\n",
    "\n",
    "### pp. 107\n",
    "\n",
    "Eq. (2.176) looks confusing, because $r=1$ is already used in it.\n",
    "\n",
    "von Mises distribution is basically checking the unit circle for a 2D Gaussian.\n",
    "\n",
    "### pp. 109\n",
    "\n",
    "I should say there's some slackness in from Eq. (2.182) to Eq. (2.184). Problems are 1) For 2.182, $\\pi + \\theta_\\mathrm{ml}$ also satisfies. 2) Sometimes (such as $\\sum \\cos(\\theta_n) = 0$), $tan$ doesn't exist. So there are some special cases to discuss, but I believe the correct result should be using `atan2` in Eq. (2.184).\n",
    "\n",
    "I didn't check the correctness of computing $m$, as I don't need it now. Checking <https://en.wikipedia.org/wiki/Von_Mises_distribution#Estimation_of_parameters>, it looks the method in PRML is also mentioned there. So it should be correct. Notice that that wiki (as of 10/14/2016) says that the ML estimator of mean direction is biased, and I don't think this is correct, as people usually talk about bias in the concentration parameter, but not mean, such as in <https://cran.r-project.org/web/packages/CircStats/CircStats.pdf>.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exponential Family\n",
    "\n",
    "For most of content in this section, check my special notes on expoenential families.\n",
    "\n",
    "### 2.4.2 Conjugate priors\n",
    "\n",
    "The interpretation of $\\nu$ as the number of pseudo observations is good. You can see that when expressing distributions in exponential family form, the picture is very clear. Notice that $\\chi$ is the value of sufficient statistics of those pseudo observations.\n",
    "\n",
    "### 2.4.3 Noninformative priors\n",
    "\n",
    "#### pp. 118\n",
    "\n",
    "> In practice, improper priors can often be used provided the corresponding posterior distribution is proper, i.e., that it can be correctly normalized.\n",
    "\n",
    "Indeed, prior being improper is fine, as long as we have data.\n",
    "\n",
    "> A second difficulty arises from the transformation behaviour of a probability density under a nonlinear change of variables\n",
    "\n",
    "This second difficulty is same as that for MAP estimation, check my note on pp. 153 of Chapter 5 of MLAPP.\n",
    "\n",
    "Later on, the author introduced two types of noninformative priors, one for location invariance, one for scale. Notice that they also suffer the \"second difficulty\" mentioned above.\n",
    "\n",
    "One quick note about Eq. (2.239). After getting it, we can set $\\sigma=1$, and we have $p(x)x=p(1)$, or $p(x)=p(1)/x$, getting the conclusion in the book $p(\\sigma) \\propto 1/\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Nonparametric Methods\n",
    "\n",
    "### pp. 125\n",
    "\n",
    "> Note that the model produced by K nearest neighbours is not a true density model because the integral over all space diverges.\n",
    "\n",
    "Probably this is the reason I never see KNN **density estimation** in practice.\n",
    "\n",
    "Later on in this page there's a derivation of KNN classifier using Bayes rule and KNN density estimator. Interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
