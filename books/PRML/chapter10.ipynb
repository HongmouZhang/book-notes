{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a collection of highlights and comments for Chapter 10 of PRML book. page numbers are probably based on first printing of the book. (I only have a PDF version of the book's (possibly) first printing).\n",
    "\n",
    "## Caveats\n",
    "\n",
    "You should really check out the errata for reading this chapter (well, same for other chapters, but this one is especially the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 10.1 Variational Inference\n",
    "\n",
    "#### pp. 463 essentials of VI\n",
    "\n",
    "Eqs. (10.2) through (10.4) are the most important ones.\n",
    "\n",
    "#### pp. 464 neural network as q distribution.\n",
    "\n",
    "> One way to restrict the family of approximating distributions is to use a parametric distribution $q(Z\\mid \\omega)$ governed by a set of parameters $\\omega$.\n",
    "\n",
    "According to Ruslan's deep learning course, $q$ can be a neural network... $\\omega$ is weights for a neural network... People call these recognition networks, featuring neural networks computing (approximately) posterior.\n",
    "\n",
    "### 10.1.1 Factorized distributions\n",
    "\n",
    "#### pp. 464 mean field theory allows interaction between variables\n",
    "\n",
    "> Suppose we partition the elements of $Z$ into disjoint groups that we denote by $Z_i$.\n",
    "\n",
    "so mean field actually allow some interaction between variables, if we don't use singleton $Z_i$.\n",
    "\n",
    "#### pp. 465 derivation of MF update equation.\n",
    "\n",
    "Here we derive Eq. (10.6)\n",
    "\n",
    "Here, we start with some initialization of $q$, and we want to revise a particular $q_j$. Notice that the product sign in $\\prod_i q_i$ only applies to $q_i$.\n",
    "\n",
    "##### first row to second row of Eq. (10.6)\n",
    "\n",
    "Essentially, we break the term $\\ln p(X,Z) - \\sum_i \\ln q_i$ into two, resulting in subtraction of two integrals.\n",
    "\n",
    "1. First integral is just first term in second row, except that the integral over $Z_i$ is separated from others.\n",
    "2. Second integral is the $\\int q_j \\ln q_j \\mathrm{d} Z_j$ plus (well should be minus, but whatever, since it's const) const stuff.\n",
    "\n",
    "Here I give you a derivation of second term.\n",
    "\n",
    "\\begin{align}\n",
    "\\int \\prod_i q_i (\\sum_k \\ln q_k) \\mathrm{d}Z &= \\int \\prod_i q_i ( \\ln q_j + \\sum_{k \\ne j} \\ln q_k) \\mathrm{d}Z \\\\\n",
    "                                              &= \\int \\prod_i q_i \\ln q_j  \\mathrm{d}Z + \\int \\prod_i q_i (\\sum_{k \\ne j} \\ln q_k) \\mathrm{d}Z \\\\\n",
    "                                              &= \\int \\left[ q_j\\ln q_j   \\prod_{i\\ne j} (\\int q_i  \\mathrm{d} Z_i) \\right]  \\mathrm{d}Z_j + \\int \\prod_i q_i (\\sum_{k \\ne j} \\ln q_k) \\mathrm{d}Z \\\\\n",
    "                                              &= \\int q_j\\ln q_j  \\mathrm{d}Z_j + \\int \\prod_i q_i (\\sum_{k \\ne j} \\ln q_k) \\mathrm{d}Z \\\\\n",
    "                                              &= \\int q_j\\ln q_j  \\mathrm{d}Z_j + \\int \\prod_{r\\ne j} q_r (\\sum_{k \\ne j} \\ln q_k)\\left[  \\int q_j \\mathrm{d}Z_j \\right]\\mathrm{d}Z_{-i} \\\\\n",
    "                                              &= \\int q_j\\ln q_j  \\mathrm{d}Z_j + \\int \\prod_{r\\ne j} q_r (\\sum_{k \\ne j} \\ln q_k)\\mathrm{d}Z_{-i} \\\\\n",
    "                                              &= \\int q_j\\ln q_j  \\mathrm{d}Z_j + const\n",
    "\\end{align}\n",
    "\n",
    "where $Z_{-i}$ means all but $i$. Here all const is relative to $j$.\n",
    "\n",
    "##### second row to third row,\n",
    "\n",
    "We replace the inner integral in first term as $\\ln \\tilde{p}(X,Z_j) + c$, and that $c$ gets consumed into $const$. Notice that indeed this $\\ln \\tilde{p}(X,Z_j)$ is only a function of $Z_j$ (we consider $X$ as fixed here), since all other dimensions are marginalized out. $\\ln \\tilde{p}(X,Z_j)$ is the normalized log score of a distribution whose (unnormalized) log is $E_{i\\ne j}[\\ln p(X,Z)]$, which is the inner integral.\n",
    "\n",
    "Thus, we have optimal $q_j$ as the equation in pp. 466, by noticing that we are optimizing KL between q and that distribution mentioned above.\n",
    "\n",
    "#### pp. 466 convergence of MF algorithm\n",
    "\n",
    "One way to show it is that 1) after each step the $\\mathcal{L}$ is not decreased; 2) there's some bound on maximally achievable $\\mathcal{L}$.\n",
    "\n",
    "> Convergence is guaranteed because bound is convex with respect to each of the factors $q_i(Z_i)$ (Boyd and Vandenberghe, 2004).\n",
    "\n",
    "\"bound is convex ...\" means $\\mathcal{L}$ is convex w.r.t each factor. This is a sufficient condition for us to show 1), as long as we can find this compute this optimal solution (which is assumed). For each factor, the cost function is convex. This can be seen by replacing integral with summation. The optimization problem at Eq. (10.6) becomes maximizing $\\sum_k q_{j,k} \\tilde{p}_k - \\sum_k q_{j, k} \\ln q_{j,k}$, with constraint  $\\sum_k q_{j,k} = 1$, where I use $k$ to index all states. Clearly this function is concave for $q_{j,k}$.\n",
    "\n",
    "Thus, at each step of MF algorithm, we can always find the optimal one, without decreasing the $\\mathcal{L}$.\n",
    "\n",
    "To show 2), this is because $\\mathcal{L}$ has maximally possible value of the true likelihood, then we have a sequence of bounded nondecreasing number, and convergence is guaranteed.\n",
    "\n",
    "However, the whole mean field problem is non convex. Check *Graphical Models, Exponential Families, and Variational Inference* by Jordan.\n",
    "\n",
    "#### pp. 468-469 two types of KL\n",
    "\n",
    "standard VI optimizes $KL(q \\| p)$, which tends to make the distribution too compact; we can also (in theory) optimize $KL(p \\| q)$, which gives too spread approximation. Check Figure 10.2 and Figure 10.3. This highlights that indeed the asymmetry of KL can have practical issue.\n",
    "\n",
    "#### pp. 469-470 two types of KL are extremes\n",
    "\n",
    "they are all alpha-family divergences. See Eq. (10.19). I guess people don't use these more exotic divergences, primarily because it's hard to compute.\n",
    "\n",
    "### 10.1.4 Model comparison\n",
    "\n",
    "First, check errata. A lot of notation here is wrong. Also, for line -3 and -2 on pp. 473, \"the solutions for different $m$ are **uncoupled**\", not coupled, since they are conditioned on $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, this chapter gives some examples of mean field approximation, in Section 10.2, 10.3, 10.4. While there are some tricky parts, such as model comparison (Section 10.2.4), they are irrelevant for Deep Learning.\n",
    "\n",
    "Section 10.5 gives another type of way to do approximation. Instead of using factorized distribution, they derive upper and lower bounds, using dual methods. Check Eq. (10.129) and later few ones. I think such methods are mentioned in Boyd's Convex Optimization book. Essentially they provide an alternative way for approximation.\n",
    "\n",
    "Section 10.7 introduces Expectation Propagation, which is optimizing $KL(p\\|q)$ instead of $KL(q\\|p)$. The algorithm is similar to mean field, as shown in pp. 509, in an interative fashion. I think this EP algorithm is related to many popular VI algorithms in PGM, such as loopy BP, etc., based checking the table of contents for MLAPP book. Again, they are irrelevant for Deep Learning, based on Deep Learning book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
