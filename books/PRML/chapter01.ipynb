{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of highlights and comments for Chapter 1 of PRML book. page numbers are probably based on first printing of the book. (I only have a PDF version of the book's (possibly) first printing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Example\n",
    "### pp. 10\n",
    "\n",
    "> Note that often the coefficient $w_0$ is omitted from the regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pp. 18\n",
    "\n",
    "> the maximum of a probability density is dependent on the choice of variable.\n",
    "\n",
    "but not the case for maximum of a function. Check the reference solution.\n",
    "\n",
    "In practice, I think it means that our probability distribution has to be \"correct\", but we can do arbitrary change of variable when optimizing a function.\n",
    "\n",
    "BTW, $p_x(x)\\delta x \\approx p_y(y) \\delta y$ is not a correct definition for change of variable, as two y might map to same x. Check pp. 50 of Kevin Murphyâ€™s MLAPP for a more rigorous proof. Basically, these things only work for invertible mappings, which is probably sufficient. In this setting, this (1.27) can be seen as integration by substitution, with some careful handling on sign.\n",
    "\n",
    "For the implication of this to MAP estimation, check my note on pp. 153 of Chapter 5 of MLAPP.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Probability Theory\n",
    "### pp. 21\n",
    "\n",
    "> rigorous proof that probability theory could be regarded as an extension of Boolean logic to situations involving uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pp. 22\n",
    "\n",
    "> One approach to determining frequentist error bars is the bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pp. 28\n",
    "\n",
    "> it is measured relative to the sample mean and not relative to the true mean.\n",
    "\n",
    "First, Figure 1.15 here is not correct. Check the correct version from book website; Second, highlighted argument applies to (a) and (c), but not (b).\n",
    "\n",
    "> In fact, as we shall see, the issue of bias in maximum likelihood lies at the root of the over-fitting problem that we encountered earlier in the context of polynomial curve fitting\n",
    "\n",
    "so the bias in parameter estimation, and bias in bias-variance tradeoff are the same concept.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model Selection\n",
    "### pp. 33\n",
    "\n",
    "> Such criteria do not take account of the uncertainty in the model parameters, however, and in practice they tend to favour overly simple models\n",
    "\n",
    "basically the author doesn't like AIC or BIC.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The Curse of Dimensionality\n",
    "### pp. 35\n",
    "\n",
    "> The problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty.\n",
    "\n",
    "### pp. 37\n",
    "\n",
    "> First, real data will often be confined to a region of the space having lower effective dimensionality, and in particular the directions over which important variations in the target variables occur may be so confined.\n",
    "\n",
    "So high dimension kernel density estimation or KNN methods may not be that bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Decision Theory\n",
    "### pp. 43\n",
    "\n",
    "> three distinct approaches to solving decision problems\n",
    "\n",
    "I think (a) and (b) are both modeling the joint distribution $p(x_i, C_i)$ ($C_i$ is class label for example $x_i$).\n",
    "\n",
    "In (a), it's done by first modeling $p(x \\mid C_k)$, and then model $p(C_k)$ by directly using proportion of $C_k$ directly.\n",
    "\n",
    "In (b), it's done by first modeling $p(C \\mid x)$, and then plug in $p(x)$ by the data points themselves.\n",
    "\n",
    "Since x is continuous (usually), and parameters of models for different $p(C \\mid x)$ (different x) are usually shared, this approach in theory doesn't train different $p(C \\mid x)$ for different x independently, so we have problems like balanced dataset, etc. discussed in the next page (**Compensating for class priors**)\n",
    "\n",
    "(a) has similar problem, but I guess the models for different $p(x\\mid C_k)$ are independent, so minimizing them together is same as individually.\n",
    "\n",
    "\n",
    "So both approaches try to model $p(C, x)$, but using different plug-in parts of the data.\n",
    "\n",
    "Well, we don't necessarily do plugin in this fashion, as you could in theory simply jointly model $p(C_k, x)$ together using a single model, and then condition to get $p(C_k \\mid x)$. I think this is what's done in the case of generative approach for regression in pp. 47. Here, modeling $p(x \\mid C_k)$ instead of together, is just what's done in practice usually.\n",
    "\n",
    "> This can be useful for detecting new data points that have low probability under the model\n",
    "\n",
    "### pp. 44\n",
    "\n",
    "> Indeed, the classconditional densities may contain a lot of structure that has little effect on the posterior probabilities, as illustrated in Figure 1.27.\n",
    "\n",
    "Why generative model can be too expensive.\n",
    "\n",
    "> **Minimizing Risk** Consider a problem in which ...\n",
    "\n",
    "Here, I guess, in the discriminative or generative approach, the loss for each class is not used in training, and it's only used in making decision. Instead, in the pure loss function approach, the loss for each class is used in training. But I should say this is just one way to do this.\n",
    "\n",
    "### pp. 45\n",
    "\n",
    "> **Compensating for class priors.**\n",
    "\n",
    "Here you create that balanced data set to make sure positive and negative examples have roughly same weight and their quality are roughly the same. If using the original data set, one class will be overwhelmed by the other, and the posterior learned will be poorer for the former class.\n",
    "\n",
    "### pp. 46\n",
    "\n",
    "> Equation (1.88)\n",
    "\n",
    "just check Appendix D. (1.88) is the stationary condition for a particular $x$. To understand this more easily, replace the integral over $x$ with a sum over many $x$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Information Theory\n",
    "### pp. 48\n",
    "\n",
    "> $h(x)$ must be given by the logarithm of $p(x)$\n",
    "\n",
    "Check solution manual. very good solution. here, their assumption is to find a mapping from $[0,1]$ to some measure of information in it.\n",
    "\n",
    "### pp. 50\n",
    "\n",
    "> The noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable.\n",
    "\n",
    "### pp. 53\n",
    "\n",
    "> This reflects the fact that to specify a continuous variable very precisely requires a large number of bits.\n",
    "\n",
    "### pp. 54\n",
    "\n",
    "> Note that we did not constrain the distribution to be nonnegative when we maximized the entropy. However, because the resulting distribution is indeed nonnegative, we see with hindsight that such a constraint is not necessary.\n",
    "\n",
    "same goes to maximization of discrete entropy, where we don't specify everything to be non negative.\n",
    "\n",
    "### pp. 55\n",
    "\n",
    "> average additional amount of information\n",
    "\n",
    "from this interpretation, it's obvious that KL must be nonnegative.\n",
    "\n",
    "### pp. 57\n",
    "\n",
    "> so the equality will hold if, and only if, $q(x) = p(x)$ for all $x$\n",
    "\n",
    "just take this as a fact.\n",
    "\n",
    "> Thus we see that minimizing this Kullback-Leibler divergence is equivalent to maximizing the likelihood function.\n",
    "\n",
    "### misc.\n",
    "\n",
    "below is a figure I copied from the reference solution manual. Very good summary of various entropy functions.\n",
    "\n",
    "![](images/entropy_graph.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
