{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models Part 2: Markov Random Fields\n",
    "\n",
    "This document is about Markov Random Fields (but not its learning or inference algorithms).\n",
    "\n",
    "\n",
    "\n",
    "## Interpretation of potential functions\n",
    "We know that a distribution based on a undirected graph is defined as the product of potential functions defined on the (maximal) cliques of the graph. There's no conditional probability or marginal probability interpretation of these potential functions, as discussed in pp. 28 and pp. 31 of Jordan's book (Chapter 2).\n",
    "\n",
    "This is also discussed in 10-708 (slides for lecture 3, pp. 19)\n",
    "![image](./interpretation_potential_functions_10708.png)\n",
    "\n",
    "## Three kinds of conditional independence statements from MRF\n",
    "\n",
    "For BN, we have **local independencies**, as well as the whole set defined by d-seperation. For MRF, three sets of independencies are there.\n",
    "\n",
    "### Global independencies\n",
    "\n",
    "This corresponds to the set defined by d-sep for BN.\n",
    "\n",
    "Koller book (pp. 115):\n",
    "![image](./indep_global_Koller.png)\n",
    "\n",
    "10708 slides (pp. 14, lecture 3): \n",
    "![image](./indep_global_10708.png)\n",
    "\n",
    "### Local independencies\n",
    "\n",
    "Koller book (pp. 118):\n",
    "![image](./indep_local_Koller.png)\n",
    "\n",
    "### Pairwise independencies\n",
    "\n",
    "Koller book (pp. 118):\n",
    "![image](./indep_pairwise_Koller.png)\n",
    "\n",
    "### Relationship of the three\n",
    "\n",
    "Clearly, each set of independencies is (literally) weaker than the previous set. But they are the same for positive distributions.\n",
    "\n",
    "Koller book (pp. 119):\n",
    "![image](./indep_equi_Koller.png)\n",
    "\n",
    "10708 slides (pp. 28, lecture 3): \n",
    "![image](./indep_equi_10708.png)\n",
    "\n",
    "## Equivalence theorem\n",
    "\n",
    "This establishes the equivalence between using graph structure and the set of conditional independencies. We only discuss **positive** distributions.\n",
    "\n",
    "More specifically, the set of **positive** distributions specified by \n",
    "\n",
    "1. factorization (P factorizes according to G)\n",
    "2. set of conditional independence statements (G is an I-map of P, or P satisfies conditional independence statements from G)\n",
    "\n",
    "are the same.\n",
    "\n",
    "Koller book (pp. 115):\n",
    "![image](./soundness_Koller.png)\n",
    "This is the soundness part in the next section.\n",
    "\n",
    "Koller book (pp. 116):\n",
    "![image](./HC_Koller.png)\n",
    "This direction (from independencies to factorization) is also called Hammersley-Clifford theorem. (Well I think there are many versions of this theorem, and I just pick this one based on Koller's book.)\n",
    "\n",
    "There's an example about why the previous statement only works for positive distributions (I don't check its correctness).\n",
    "\n",
    "![image](./HC_why_positive_Koller.png)\n",
    "\n",
    "\n",
    "## Soundness and completeness\n",
    "\n",
    "This is about whether the set of conditional independence statements inferred from a MRF graph using sep is the (maximum) set of conditional independence statements applicable to every **positive** P factorized according to the given MRF graph.\n",
    "\n",
    "Soundness: \n",
    "\n",
    "Koller book (pp. 115):\n",
    "![image](./soundness_Koller.png)\n",
    "\n",
    "Completeness:\n",
    "![image](./completeness_Koller.png)\n",
    "\n",
    "Stronger version of completeness: \n",
    "![image](./completeness_Koller_2.png)\n",
    "\n",
    "Soundness + Completeness (10708 slides for lecture 3, pp. 25)\n",
    "![image](./soundness_completeness_10708.png)\n",
    "\n",
    "## Alternative, compact representations of MRF\n",
    "\n",
    "### Factor graph\n",
    "\n",
    "This representation of MRF makes explicit the factors (clique potentials) involved in the network, and can reveal some fine-grained strucutres, although not in terms of additional conditional indepdendence statements. Basically, same MRF can map to different factor graphs. Factor graph can be helpful in inference (Junction Tree algorithm).\n",
    "\n",
    "One example in Koller's book (pp. 123):\n",
    "![image](./factor_graph_Koller.png)\n",
    "\n",
    "FG is also discussed in pp. 36 of Jordan's book (chapter 2). Jordan mentioned that factorization is a richer concept than conditional independence, since different FG can map to the same MRF.\n",
    "\n",
    "\n",
    "### Log-linear models\n",
    "\n",
    "In specific application contexts, we may like to represent MRF factorization using summation.\n",
    "\n",
    "Koller's book (pp. 124-125):\n",
    "![image](./log_linear_Koller_1.png)\n",
    "![image](./log_linear_Koller_2.png)\n",
    "\n",
    "I think in many cases, people just assume features are known and only learn $w_i$.\n",
    "\n",
    "In theory log linear model can represent all positive distributions, as long as you use correct features.\n",
    "\n",
    "## Other stuffs\n",
    "\n",
    "Koller and 10708 also discuss minimal I-map, etc.  But they are mainly about how to learn a MRF from a set of conditional independencies in the data. Currently I'm not interested in such topics, so I just skip them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
